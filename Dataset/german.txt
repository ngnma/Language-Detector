Technologie im heutigen Sinne ist die Wissenschaft und Lehre von der Technik zur Planung und Herstellung von Industrieprodukten. Sie erforscht und vermittelt im Wesentlichen technisches Know-how zur Gestaltung verfahrens- und anwendungstechnischer Prozesse in Industriebetrieben einschließlich der Planung und Bereitstellung der erforderlichen Betriebsmittel und technischen Dokumentationen.
Etymologie[Bearbeiten | Quelltext bearbeiten]
Das Wort Technologie leitet sich ab von altgriechisch τεχνολογία technología „kunstgemäße Abhandlung über eine Kunst oder Wissenschaft“,[1] das seinerseits auf τέχνη téchnē „Kunst, Handwerk“[2] sowie λόγος lógos (hier wie lateinisch litterae in der Bedeutung „Wissenschaften“, vergleiche -logie)[3] zurückgeht. Im hellenistischen Griechisch (Koine, ab ca. 300 v. Chr.) wurde damit gelegentlich die „systematische Behandlung der Grammatik und Rhetorik“ bezeichnet.[4]

Allgemeines[Bearbeiten | Quelltext bearbeiten]
Der Begriffsinhalt hat sich im Laufe der Zeit verschoben. Das Wort besaß früher die Bedeutung einer Kunstlehre zur Gewerbekunde.[5] In neuerer Zeit überwiegen Bedeutungen wie „Lehre vom Handwerk“, „Wissenschaft von der Technik“ oder „technisches Know-how“, doch unterscheiden sich die verschiedenen Begriffsauffassungen teilweise beträchtlich. Technologie ist insgesamt betrachtet naturwissenschaftlich-technisches Wissen, welches die Grundlage für Produkte und Produktionsverfahren darstellt.[6]

Technischer Fortschritt bringt Produkt- oder Finanzinnovationen hervor; wer einen Vorsprung in der Anwendung neuer Technologien gegenüber der Konkurrenz aufweist, heißt Technologieführer.
19. Jahrhundert[Bearbeiten | Quelltext bearbeiten]
Diese Auffassung steht bei Karl Marx im Vordergrund, dem es vor allem um das Verhältnis von Industriearbeit und Kapital geht. „Das Prinzip [der modernen Industrie], jeden Produktionsprozeß […] in seine konstituierenden Elemente aufzulösen, schuf die ganz moderne Wissenschaft der Technologie“.[12] Andererseits entwickelt er aber auch eine sehr viel weitergehende, sozusagen gesellschaftstheoretische Vorstellung: „Die Technologie enthüllt das aktive Verhalten des Menschen zur Natur, den unmittelbaren Produktionsprozeß seines Lebens, damit auch seiner gesellschaftlichen Lebensverhältnisse und der ihnen entquellenden geistigen Vorstellungen“.[13]

Seit dem späten 19. Jahrhundert wird Technologie zum Sondergebiet der Technikwissenschaften, das sich mit den Ver- und Bearbeitungsverfahren befasst. Man spricht von mechanischer, chemischer, Lebensmitteltechnologie usw., schränkt den Begriff also ausdrücklich auf die Lehre von den Produktionsverfahren ein.
Universitäten (vom lateinischen universitas magistrorum et scolarium,[3] „Gemeinschaft der Lehrer und Schüler“, später im Sinne Humboldts für universitas litterarum, „Gesamtheit der Wissenschaften“) sind Hochschulen mit Promotionsrecht, die der Pflege und Entwicklung der Wissenschaften durch Forschung, Lehre und Studium dienen[4] aber ihren Studenten auch praxisorientiert Berufsqualifikationen[5] vermitteln sollen. Neben den Volluniversitäten, die ein breites Fächerspektrum (Universalität) anbieten und mehrere zehntausend Studierende haben können (Massenuniversität), gibt es auch kleinere staatliche und Privatuniversitäten, die meist auf wenige Fächer spezialisiert sind, und deren Anzahl an Immatrikulierten eher im vierstelligen Bereich liegt.

Als älteste Universität der Welt im modernen Sinne wird gemeinhin die Universität Bologna (1088) in Italien gezählt (ein Prototyp war die Schule von Salerno), wohingegen die Universität Heidelberg (1386) die älteste Deutschlands, die Universität Wien (1365) die älteste Österreichs und die Universität Basel (1460) die älteste der Schweiz darstellen. Die Universität Oxford (ca. 1096) ist die älteste englische Universität, die Universität Harvard (1636) die älteste in den Vereinigten Staaten.
Die Bezeichnung Universität (von lateinisch universitas ‚Gesamtheit‘) charakterisiert begrifflich im Wissenschaftsbereich ganz allgemein eine umfassende Bildungseinrichtung.[6] An den damals neu gegründeten Institutionen von Bologna (gegründet 1088), Paris (gegründet um 1150) oder Oxford (gegründet im 12. Jahrhundert) studierte man im heutigen Sinne eines Studium generale. Es handelte sich um eine noch überschaubare Anzahl wissenschaftlicher Disziplinen (septem artes liberales ‚Sieben Freie Künste‘, ergänzt durch Theologie, Jurisprudenz und Medizin). Die Gesamtheit dieser Wissenschaften fasste man später unter der Bezeichnung universitas litterarum (‚Gesamtheit der Wissenschaften‘). Vor allem durch Wilhelm von Humboldt, der die Einheit von Lehre und Forschung zum Grundprinzip universitärer Arbeit erhob, wurde dieser Begriff für die moderne Universität prägend. Daneben trat das ursprüngliche Verständnis von universitas, das aus den korporativen Organisationsformen mittelalterlicher Lehr- und Lerngemeinschaften (universitas magistrorum et scholarium ‚Gemeinschaft der Lehrenden und Lernenden‘) im Bereich bedeutender kirchlicher Bildungszentren erwachsen war, etwas in den Hintergrund. Es lebt aber im Begriff der Autonomie der Hochschulen weiter.

Mit der zunehmenden Ausdifferenzierung und Vermehrung der Wissenschaftsgebiete hat sich die an das Studium generale anknüpfende Begriffsfüllung überlebt, da heute keine einzelne Institution mehr die Gesamtheit der Wissenschaften vertreten kann. Insofern ist der Begriff Universität nur noch für die Gesamtheit sämtlicher, großenteils spezialisierter, Hochschulen sinnvoll verwendbar. Die auf die akademische Gemeinschaft ausgerichtete Begriffsfüllung hat ebenfalls ihren ursprünglichen Anwendungsort verloren und ihren Sinn erweitert, da diese Bedeutung für alle Hochschulen, also beispielsweise auch für die Fachhochschulen, zutrifft.
Nach dem Grundgesetz ist die Hochschulgesetzgebung grundsätzlich Sache der Länder. Dies entspricht, wenn man von der zentralistischen Zeit des Dritten Reichs oder der DDR absieht, auch der historischen Entwicklung in Deutschland. Fast alle alten Universitäten wurden von den Landesfürsten errichtet, die dazu allerdings ein Kaiserliches Privileg benötigten. Aus Gründen der Hochschulfinanzierung kam es jedoch auch zu rahmengesetzlichen Regelungen durch den Bund mit dem Hochschulrahmengesetz. Aufgrund der Föderalismusreform wird die Aufhebung des Hochschulrahmengesetzes angestrebt.[10] Ansonsten müssen sich die Länder untereinander staatsvertraglich über gemeinsam gewollte oder nicht gewollte Sachverhalte verständigen, was in der Regel im Rahmen der Kultusministerkonferenz stattfindet. Auch dies hat historische Dimension: bereits 1654 trafen die evangelischen Reichsstände auf dem Reichstag zu Regensburg ein erstes Abkommen zur Eindämmung des damals ausufernden Pennalismus an den Universitäten. Das Grundgesetz wurde dahingehend geändert, dass Bund und Länder bei bestimmten Aufgaben zusammenarbeiten können.[11]
Ein Computer (englisch; deutsche Aussprache [kɔmˈpjuːtɐ]) oder Rechner ist ein Gerät, das mittels programmierbarer Rechenvorschriften Daten verarbeitet. Dementsprechend sind vereinzelt auch die abstrahierenden bzw. veralteten, synonym gebrauchten Begriffe Rechenanlage, Datenverarbeitungsanlage oder elektronische Datenverarbeitungsanlage sowie Elektronengehirn anzutreffen.

Charles Babbage und Ada Lovelace gelten durch die von Babbage 1837 entworfene Rechenmaschine Analytical Engine als Vordenker des modernen universell programmierbaren Computers. Konrad Zuse (Z3, 1941 und Z4, 1945) in Berlin, John Presper Eckert und John William Mauchly (ENIAC, 1946) bauten die ersten funktionstüchtigen Geräte dieser Art. Bei der Klassifizierung eines Geräts als universell programmierbarer Computer spielt die Turing-Vollständigkeit eine wesentliche Rolle. Sie ist benannt nach dem englischen Mathematiker Alan Turing, der 1936 das logische Modell der Turingmaschine eingeführt hatte.[1][2]

Die frühen Computer wurden auch (Groß-)Rechner genannt; ihre Ein- und Ausgabe der Daten war zunächst auf Zahlen beschränkt. Zwar verstehen sich moderne Computer auf den Umgang mit weiteren Daten, beispielsweise mit Buchstaben und Tönen. Diese Daten werden jedoch innerhalb des Computers in Zahlen umgewandelt und als solche verarbeitet, weshalb ein Computer auch heute eine Rechenmaschine ist.

Mit zunehmender Leistungsfähigkeit eröffneten sich neue Einsatzbereiche. Computer sind heute in allen Bereichen des täglichen Lebens vorzufinden, meistens in spezialisierten Varianten, die auf einen vorliegenden Anwendungszweck zugeschnitten sind. So dienen integrierte Kleinstcomputer (eingebettetes System) zur Steuerung von Alltagsgeräten wie Waschmaschinen und Videorekordern oder zur Münzprüfung in Warenautomaten; in modernen Automobilen dienen sie beispielsweise zur Anzeige von Fahrdaten und steuern in „Fahrassistenten“ diverse Manöver selbst.

Universelle Computer finden sich in Smartphones und Spielkonsolen. Personal Computer (engl. für Persönliche Computer, als Gegensatz zu von vielen genutzten Großrechnern) dienen der Informationsverarbeitung in Wirtschaft und Behörden sowie bei Privatpersonen; Supercomputer werden eingesetzt, um komplexe Vorgänge zu simulieren, z. B. in der Klimaforschung oder für medizinische Berechnungen.
Computer
Das englische Substantiv computer ist abgeleitet von dem englischen Verb to compute. Jenes ist abgeleitet von dem lateinischen Verb computare, was zusammenrechnen bedeutet.

Der englische Begriff computer war ursprünglich eine Berufsbezeichnung für Hilfskräfte, die immer wiederkehrende Berechnungen (z. B. für die Astronomie, für die Geodäsie oder für die Ballistik) im Auftrag von Mathematikern ausführten und damit Tabellen wie z. B. eine Logarithmentafel füllten.

In der frühen Kirchengeschichte erfolgte eine Ablösung des jüdischen Kalenders durch den Julianischen Kalender. Die hieraus resultierenden Berechnungsschwierigkeiten des Osterdatums dauerten bis zum Mittelalter an und waren Gegenstand zahlreicher Publikationen, häufig betitelt mit Computus Ecclesiasticus. Doch finden sich noch weitere Titel, z. B. von Sigismund Suevus 1574, die sich mit arithmetischen Fragestellungen auseinandersetzen. Der früheste Text, in dem das Wort Computer isoliert verwendet wird, stammt von 1613.[3]

In der Zeitung The New York Times tauchte das Wort erstmals am 2. Mai 1892 in einer Kleinanzeige der United States Navy mit dem Titel A Computer Wanted ‚Ein Rechner gesucht‘ auf, in der Kenntnisse in Algebra, Geometrie, Trigonometrie und Astronomie vorausgesetzt worden sind.[4]

An der University of Pennsylvania in Philadelphia wurden im Auftrag der United States Army ballistische Tabellen berechnet. Das Ergebnis waren Bücher für die Artillerie, die für unterschiedliche Geschütze Flugbahnen unterschiedlicher Geschosse vorhersagten. Diese Berechnungen erfolgten größtenteils von Hand. Die einzige Hilfe war eine Tabelliermaschine, die zu multiplizieren und zu dividieren vermochte. Die Angestellten, die dort rechneten, wurden „computer“ (im Sinne eines menschlichen Computers) genannt.[5][6] Erstmals wurde der Begriff 1946 bei der dort entwickelten elektronischen Rechenanlage Electronic Numerical Integrator and Computer (ENIAC) für ein technisches Gerät verwendet. Seit 1962 ist der Begriff in Deutschland belegt.[7]
Grundsätzlich unterscheiden sich zwei Bauweisen: Ein Rechner ist ein Digitalrechner, wenn er mit digitalen Geräteeinheiten digitale Daten verarbeitet (also Zahlen und Textzeichen); er ist ein Analogrechner, wenn er mit analogen Geräteeinheiten analoge Daten verarbeitet (also kontinuierlich verlaufende elektrische Messgrößen wie Spannung oder Strom).

Heute werden fast ausschließlich Digitalrechner eingesetzt. Diese folgen gemeinsamen Grundprinzipien, mit denen ihre freie Programmierung ermöglicht wird. Bei einem Digitalrechner werden dabei zwei grundsätzliche Bestandteile unterschieden: Die Hardware, die aus den elektronischen, physisch anfassbaren Teilen des Computers gebildet wird, sowie die Software, die die Programmierung des Computers beschreibt.

Ein Digitalrechner besteht zunächst nur aus Hardware. Die Hardware stellt erstens einen Speicher bereit, in dem Daten portionsweise wie auf den nummerierten Seiten eines Buches gespeichert und jederzeit zur Verarbeitung oder Ausgabe abgerufen werden können. Zweitens verfügt das Rechenwerk der Hardware über grundlegende Bausteine für eine freie Programmierung, mit denen jede beliebige Verarbeitungslogik für Daten dargestellt werden kann: Diese Bausteine sind im Prinzip die Berechnung, der Vergleich und der bedingte Sprung. Ein Digitalrechner kann beispielsweise zwei Zahlen addieren, das Ergebnis mit einer dritten Zahl vergleichen und dann abhängig vom Ergebnis entweder an der einen oder der anderen Stelle des Programms fortfahren. In der Informatik wird dieses Modell theoretisch durch die eingangs erwähnte Turing-Maschine abgebildet; die Turing-Maschine stellt die grundsätzlichen Überlegungen zur Berechenbarkeit dar.

Erst durch eine Software wird der Digitalcomputer jedoch nützlich. Jede Software ist im Prinzip eine definierte, funktionale Anordnung der oben geschilderten Bausteine Berechnung, Vergleich und bedingter Sprung, wobei die Bausteine beliebig oft verwendet werden können. Diese Anordnung der Bausteine, die als Programm bezeichnet wird, wird in Form von Daten im Speicher des Computers abgelegt. Von dort kann sie von der Hardware ausgelesen und abgearbeitet werden. Dieses Funktionsprinzip der Digitalcomputer hat sich seit seinen Ursprüngen in der Mitte des 20. Jahrhunderts nicht wesentlich verändert, wenngleich die Details der Technologie erheblich verbessert wurden.

Analogrechner funktionieren nach einem anderen Prinzip. Bei ihnen ersetzen analoge Bauelemente (Verstärker, Kondensatoren) die Logikprogrammierung. Analogrechner wurden früher häufiger zur Simulation von Regelvorgängen eingesetzt (siehe: Regelungstechnik), sind heute aber fast vollständig von Digitalcomputern abgelöst worden. In einer Übergangszeit gab es auch Hybridrechner, die einen Analog- mit einem digitalen Computer kombinierten.
In den heutigen Computern sind die ALU und die Steuereinheit meistens zu einem Baustein verschmolzen, der so genannten CPU (Central Processing Unit, zentraler Prozessor).

Der Speicher ist eine Anzahl von durchnummerierten, adressierbaren „Zellen“; jede von ihnen kann ein einzelnes Stück Information aufnehmen. Diese Information wird als Binärzahl, also eine Abfolge von ja/nein-Informationen im Sinne von Einsen und Nullen, in der Speicherzelle abgelegt.

Bezüglich des Speicherwerks ist eine wesentliche Designentscheidung der Von-Neumann-Architektur, dass sich Programm und Daten einen Speicherbereich teilen (dabei belegen die Daten in aller Regel den unteren und die Programme den oberen Speicherbereich). Demgegenüber stehen in der Harvard-Architektur Daten und Programmen eigene (physikalisch getrennte) Speicherbereiche zur Verfügung. Der Zugriff auf die Speicherbereiche kann parallel realisiert werden, was zu Geschwindigkeitsvorteilen führt. Aus diesem Grund werden digitale Signalprozessoren häufig in Harvard-Architektur ausgeführt. Weiterhin können Daten-Schreiboperationen in der Harvard-Architektur keine Programme überschreiben (Informationssicherheit).

In der Von-Neumann-Architektur ist das Steuerwerk für die Speicherverwaltung in Form von Lese- und Schreibzugriffen zuständig.

Die ALU hat die Aufgabe, Werte aus Speicherzellen zu kombinieren. Sie bekommt die Werte von der Steuereinheit geliefert, verrechnet sie (addiert beispielsweise zwei Zahlen) und gibt den Wert an die Steuereinheit zurück, die den Wert dann für einen Vergleich verwenden oder in eine andere Speicherzelle schreiben kann.

Die Ein-/Ausgabeeinheiten schließlich sind dafür zuständig, die initialen Programme in die Speicherzellen einzugeben und dem Benutzer die Ergebnisse der Berechnung anzuzeigen.
Die Von-Neumann-Architektur ist gewissermaßen die unterste Ebene des Funktionsprinzips eines Computers oberhalb der elektrophysikalischen Vorgänge in den Leiterbahnen. Die ersten Computer wurden auch tatsächlich so programmiert, dass man die Nummern von Befehlen und von bestimmten Speicherzellen so, wie es das Programm erforderte, nacheinander in die einzelnen Speicherzellen schrieb. Um diesen Aufwand zu reduzieren, wurden Programmiersprachen entwickelt. Diese generieren die Zahlen innerhalb der Speicherzellen, die der Computer letztlich als Programm abarbeitet, aus Textbefehlen heraus automatisch, die auch für den Programmierer einen semantisch verständlichen Inhalt darstellen (z. B. GOTO für den „unbedingten Sprung“).

Später wurden bestimmte sich wiederholende Prozeduren in so genannten Bibliotheken zusammengefasst, um nicht jedes Mal das Rad neu erfinden zu müssen, z. B.: das Interpretieren einer gedrückten Tastaturtaste als Buchstabe „A“ und damit als Zahl „65“ (im ASCII-Code). Die Bibliotheken wurden in übergeordneten Bibliotheken gebündelt, welche Unterfunktionen zu komplexen Operationen verknüpfen (Beispiel: die Anzeige eines Buchstabens „A“, bestehend aus 20 einzelnen schwarzen und 50 einzelnen weißen Punkten auf dem Bildschirm, nachdem der Benutzer die Taste „A“ gedrückt hat).

In einem modernen Computer arbeiten sehr viele dieser Programmebenen über- bzw. untereinander. Komplexere Aufgaben werden in Unteraufgaben zerlegt, die von anderen Programmierern bereits bearbeitet wurden, die wiederum auf die Vorarbeit weiterer Programmierer aufbauen, deren Bibliotheken sie verwenden. Auf der untersten Ebene findet sich aber immer der so genannte Maschinencode – jene Abfolge von Zahlen, mit der der Computer auch tatsächlich gesteuert wird.
Computersystem
Als Computersystem bezeichnet man:

ein Netzwerk oder einen Verbund aus mehreren Computern, die individuell gesteuert werden und auf gemeinsam genutzte Daten und Geräte zugreifen können;
die einen einzelnen voll funktionstüchtigen Rechner in ihrem Zusammenspiel bedingende Gesamtheit von externen und internen Komponenten, d. h. Hardware, Software wie auch angeschlossenen Peripheriegeräten;
ein System von Programmen zur Steuerung und Überwachung von Computern.[8]
Zukünftige Entwicklungen bestehen voraussichtlich aus der möglichen Nutzung biologischer Systeme (Biocomputer), weiteren Verknüpfungen zwischen biologischer und technischer Informationsverarbeitung, optischer Signalverarbeitung und neuen physikalischen Modellen (Quantencomputer).

Ein Megatrend ist derzeit (2017) die Entwicklung künstlicher Intelligenz. Hier simuliert man die Vorgänge im menschlichen Gehirn und erschafft so selbstlernende Computer, die nicht mehr wie bislang programmiert werden, sondern mit Daten trainiert werden ähnlich einem Gehirn. Der Zeitpunkt, an dem künstliche Intelligenz die menschliche Intelligenz übertrifft, nennt man technologische Singularität. Künstliche Intelligenz wird heute (2017) bereits in vielen Anwendungen, auch alltäglichen, eingesetzt (s. Anwendungen der künstlichen Intelligenz). Hans Moravec bezifferte die Rechenleistung des Gehirns auf 100 Teraflops, Raymond Kurzweil auf 10.000 Teraflops. Diese Rechenleistung haben Supercomputer bereits deutlich überschritten. Zum Vergleich liegt eine Grafikkarte für 800 Euro (5/2016) bei einer Leistung von 10 Teraflops.[9] Vier Jahre später (Dezember 2020) besitzen bereits Videospielkonsolen für ca. 500 € vergleichbare Leistung.

Für weitere Entwicklungen und Trends, von denen viele noch den Charakter von Schlagwörtern bzw. Hypes haben, siehe Autonomic Computing (= Rechnerautonomie), Grid Computing, Cloud Computing, Pervasive Computing, ubiquitäres Computing (= Rechnerallgegenwart) und Wearable Computing.

Die weltweite Websuche nach dem Begriff „Computer“ nimmt seit Beginn der Statistik 2004 stetig ab. In den zehn Jahren bis 2014 war diese Zugriffszahl auf ein Drittel gefallen.[10]
Maschinelles Lernen ist ein Oberbegriff für die „künstliche“ Generierung von Wissen aus Erfahrung: Ein künstliches System lernt aus Beispielen und kann diese nach Beendigung der Lernphase verallgemeinern. Dazu bauen Algorithmen beim maschinellen Lernen ein statistisches Modell auf, das auf Trainingsdaten beruht. Das heißt, es werden nicht einfach die Beispiele auswendig gelernt, sondern Muster und Gesetzmäßigkeiten in den Lerndaten erkannt. So kann das System auch unbekannte Daten beurteilen (Lerntransfer) oder aber am Lernen unbekannter Daten scheitern (Überanpassung; englisch overfitting).[1][2] Aus dem weiten Spektrum möglicher Anwendungen seien hier genannt: automatisierte Diagnose­verfahren, Erkennung von Kreditkartenbetrug, Aktienmarkt­analysen, Klassifikation von Nukleotidsequenzen, Sprach- und Texterkennung sowie autonome Systeme.

Das Thema ist eng verwandt mit „Knowledge Discovery in Databases“ und „Data-Mining“, bei dem es jedoch vorwiegend um das Finden von neuen Mustern und Gesetzmäßigkeiten geht. Viele Algorithmen können für beide Zwecke verwendet werden. Methoden der „Knowledge Discovery in Databases“ können genutzt werden, um Lerndaten für „maschinelles Lernen“ zu produzieren oder vorzuverarbeiten. Im Gegenzug dazu finden Algorithmen aus dem maschinellen Lernen beim Data-Mining Anwendung. Zu unterscheiden ist der Begriff zudem von dem Begriff „Deep Learning“, welches nur eine mögliche Lernvariante mittels künstlicher neuronaler Netze darstellt.
Symbolische und nicht-symbolische Ansätze[Bearbeiten | Quelltext bearbeiten]
Beim maschinellen Lernen spielen Art und Mächtigkeit der Wissensrepräsentation eine wichtige Rolle. Man unterscheidet zwischen symbolischen Ansätzen, in denen das Wissen – sowohl die Beispiele als auch die induzierten Regeln – explizit repräsentiert ist, und nicht-symbolischen Ansätzen, wie neuronalen Netzen, denen zwar ein berechenbares Verhalten „antrainiert“ wird, die jedoch keinen Einblick in die erlernten Lösungswege erlauben; hier ist Wissen implizit repräsentiert.[3]

Bei den symbolischen Ansätzen werden aussagenlogische und prädikatenlogische Systeme unterschieden. Vertreter der ersteren sind ID3 und sein Nachfolger C4.5. Letztere werden im Bereich der induktiven logischen Programmierung entwickelt.
Überwachtes Lernen[Bearbeiten | Quelltext bearbeiten]
Der Algorithmus lernt eine Funktion aus gegebenen Paaren von Ein- und Ausgaben. Dabei stellt während des Lernens ein „Lehrer“ den korrekten Funktionswert zu einer Eingabe bereit. Ziel beim überwachten Lernen ist, dass dem Netz nach mehreren Rechengängen mit unterschiedlichen Ein- und Ausgaben die Fähigkeit antrainiert wird, Assoziationen herzustellen. Ein Teilgebiet des überwachten Lernens ist die automatische Klassifizierung. Ein Anwendungsbeispiel wäre die Handschrifterkennung.

Es lassen sich noch einige Unterkategorien für Überwachtes Lernen identifizieren, die in der Literatur häufiger erwähnt werden:
Unüberwachtes Lernen[Bearbeiten | Quelltext bearbeiten]
Der Algorithmus erzeugt für eine gegebene Menge von Eingaben ein statistisches Modell, das die Eingaben beschreibt und erkannte Kategorien und Zusammenhänge enthält und somit Vorhersagen ermöglicht. Dabei gibt es Clustering-Verfahren, die die Daten in mehrere Kategorien einteilen, die sich durch charakteristische Muster voneinander unterscheiden. Das Netz erstellt somit selbständig Klassifikatoren, nach denen es die Eingabemuster einteilt. Ein wichtiger Algorithmus in diesem Zusammenhang ist der EM-Algorithmus, der iterativ die Parameter eines Modells so festlegt, dass es die gesehenen Daten optimal erklärt. Er legt dabei das Vorhandensein nicht beobachtbarer Kategorien zugrunde und schätzt abwechselnd die Zugehörigkeit der Daten zu einer der Kategorien und die Parameter, die die Kategorien ausmachen. Eine Anwendung des EM-Algorithmus findet sich beispielsweise in den Hidden Markov Models (HMMs). Andere Methoden des unüberwachten Lernens, z. B. Hauptkomponentenanalyse, verzichten auf die Kategorisierung. Sie zielen darauf ab, die beobachteten Daten in eine einfachere Repräsentation zu übersetzen, die sie trotz drastisch reduzierter Information möglichst genau wiedergibt.

Des Weiteren unterscheidet man zwischen Batch-Lernen, bei dem alle Eingabe/Ausgabe-Paare gleichzeitig vorhanden sind, und kontinuierlichem (sequentiellem) Lernen, bei dem sich die Struktur des Netzes zeitlich versetzt entwickelt.

Außerdem unterscheidet man zwischen Off-line-Lernen, bei dem alle Daten gespeichert sind und somit wiederholbar zugreifbar sind, und On-line-Lernen, bei dem die Daten nach einmaligem Ausführen und Anpassen der Gewichte verloren gehen. Batch Training ist immer off-line, On-line-Training ist immer inkrementell. Inkrementelles Lernen kann jedoch on-line oder off-line erfolgen.[8]
Iran, auch: der Iran (mit Artikel),[6] persisch ايران, DMG Īrān, Zum Anhören bitte klicken!Abspielen [ʔiːˈɾɒːn], Vollform: Islamische Republik Iran, vor 1935 auf internationaler Ebene (exonym) auch Persien, ist ein Staat in Vorderasien. Mit rund 83 Millionen Einwohnern (Stand 2019)[7] und einer Fläche von 1.648.195 Quadratkilometern zählt Iran zu den 20 bevölkerungsreichsten und größten Staaten der Erde. Hauptstadt, größte Stadt und wirtschaftlich-kulturelles Zentrum Irans ist Teheran, weitere Millionenstädte sind Maschhad, Isfahan, Täbris, Karadsch, Schiras, Ahvaz und Ghom. Der Iran bezeichnet sich selbst seit der Islamischen Revolution 1979 als Islamische Republik.

Iran besteht großteils aus hohem Gebirge und trockenen, wüstenhaften Becken. Seine Lage zwischen dem Kaspischen Meer und der Straße von Hormus am Persischen Golf macht ihn zu einem Gebiet von hoher geostrategischer Bedeutung mit langer, bis in die Antike zurückreichender Geschichte.

Nachdem sich zwischen 3200 und 2800 v. Chr. das Reich Elam gebildet hatte, vereinigten die iranischen Meder das Gebiet um 625 v. Chr. erstmals zu einem Staat, der die kulturelle und politische Führerschaft in der Region übernahm. Die von Kyros begründete Dynastie der Achämeniden regierte von Südiran aus das bis dato größte Reich der Geschichte. Es wurde im Jahre 330 v. Chr. durch die Truppen Alexanders des Großen zerstört. Nach Alexander teilten seine Nachfolger (Diadochen) das Reich unter sich auf, bis sie im iranischen Bereich um die Mitte des 3. Jahrhunderts v. Chr. durch die Parther abgelöst wurden. Auf diese folgte ab etwa 224 n. Chr. das Reich der Sassaniden, das bis zum 7. Jahrhundert neben dem Byzantinischen Reich zu den mächtigsten Staaten der Welt zählte. Nach dem Übergreifen der islamischen Expansion auf Persien, in deren Verlauf der Zoroastrismus durch den Islam ersetzt wurde, wurden persische Gelehrte zu Trägern des Goldenen Zeitalters, bis der Mongolensturm im 13. Jahrhundert das Land in seiner Entwicklung weit zurückwarf.
Der Iran grenzt an sieben Staaten: im Westen und Nordwesten an den Irak (Grenzlinie 1609 Kilometer), die Türkei (511 Kilometer), Aserbaidschan (800 Kilometer) und Armenien (48 Kilometer), im Nordosten und Osten an Turkmenistan (1205 Kilometer) sowie im Osten und Südosten an Afghanistan (945 Kilometer) und Pakistan (978 Kilometer).

Der nördlichste Punkt des Iran liegt auf 39° 47′ nördlicher Breite und befindet sich etwa auf dem selben Breitengrad wie Palma (Spanien). Der südlichste Punkt liegt auf 25° nördlicher Breite und befindet sich etwa auf dem selben Breitengrad wie Doha (Katar). Der westlichste Punkt liegt auf 44° 02′ östlicher Länge und damit etwa auf selber Länge wie Bagdad (Irak). Der östlichste Punkt liegt auf 63° 20′ östlicher Länge und damit ungefähr auf selber Länge wie Herat (Afghanistan).
Der Koran (so die eingedeutschte Form von arabisch القرآن al-Qur'ān, DMG al-Qurʾān ‚die Lesung, Rezitation‘, [al-qurˈʔaːn]) ist die heilige Schrift des Islams, die gemäß dem Glauben der Muslime die wörtliche Offenbarung Gottes (arabisch الله, Allah) an den Propheten Mohammed enthält. Er ist in einer speziellen Reimprosa abgefasst, die auf Arabisch als Sadschʿ bezeichnet wird. Der Koran besteht aus 114 Suren, diese bestehen wiederum aus einer unterschiedlichen Anzahl an Versen (آيات / āya, pl. āyāt). Maßgeblich für alle modernen Ausgaben ist die orthographisch standardisierte Kairiner Koranausgabe der Kairoer Azhar-Universität von 1924.

Ein wichtiges Kennzeichen des Korans ist seine Selbstreferentialität.[1] Das bedeutet, dass der Koran sich an vielen Stellen selbst thematisiert. Auch die meisten Glaubenslehren der Muslime hinsichtlich des Korans stützen sich auf solche selbstreferentiellen Aussagen im Koran[2]. Nach dem Glauben der sunnitischen Muslime ist der Koran die unerschaffene Rede Gottes oder zumindest ein Ausdruck davon. Eine Minderheit von Muslimen ist dagegen der Auffassung, dass der Koran erschaffen ist.
Während Suren und Verse eine sehr unterschiedliche Länge aufweisen, gibt es noch verschiedene andere Einteilungen des Korans, die den Text in gleich lange Abschnitte gliedern. Sie finden vor allem in der Liturgie Verwendung und dienen als Maßeinheiten zur Festlegung von Gebetspensen. Die wichtigsten derartigen Maßeinheiten sind der 30. Teil des Korans, Dschuzʾ genannt (جزء / ǧuzʾ, Plural أجزاء / aǧzāʾ), und der 60. Teil des Korans, Hizb genannt (حزب / ḥizb, Plural أحزاب / aḥzāb). Die Grenzen zwischen den einzelnen Dschuzʾ- und Hizb-Abschnitten befinden sich meistens mitten in einer Sure.

Die Einteilung des Korans in dreißig Teile ist besonders für den Ramadan-Monat wichtig, denn es ist eine beliebte Praxis, verteilt auf die dreißig Ramadan-Nächte eine Chatma, also eine Komplettlesung des Korans, vorzunehmen. Dschuzʾ- und Hizb-Einteilungen sind üblicherweise an den Rändern der Koranexemplare markiert, manchmal sind sogar die einzelnen Viertel des Hizb gekennzeichnet.[13]

Für die gemeinsame Koranrezitation bei feierlichen Anlässen wurden in vormoderner Zeit die Dschuzʾ-Abschnitte auch häufig einzeln abgeheftet und in einem speziellen Holzkasten, der Rabʿa genannt wurde, untergebracht. Verschiedene muslimische Herrscher wie Sultan Kait-Bay oder Sultan Murad III. ließen derartige Rabʿa-Kästen in kostbarer Ausführung anfertigen und stifteten sie den heiligen Stätten in Mekka, Medina und Jerusalem.[14] Dort waren ausgebildete Koranleser damit beauftragt, täglich daraus zu rezitieren.[15]
Vor dem Tod des Propheten Mohammed waren bereits verschiedene Teile des Korans niedergeschrieben worden, und nach Abstimmung mit allen, die den Koran sowohl mündlich (Hāfiz) als auch schriftlich bewahrt hatten, entstand nach Mohammeds Tod im Jahre 11 n. H. (632 n. Chr.) zu Zeiten des ersten Kalifen Abū Bakr der erste Koran-Kodex (مصحف muṣḥaf), um ihn vor dem Verlorengehen oder Verwechseln mit anderen Aussagen des Propheten Mohammed zu bewahren.
France (franz., dt.: Frankreich) war eine französische Exil-Tageszeitung, die während des Zweiten Weltkrieges in London herausgegeben wurde.

Das Blatt wurde nach der Besetzung Frankreichs durch die deutsche Wehrmacht im Juni 1940 durch die französischen Exilanten Pierre Comert und Georges Gombault gegründet. Chefredakteur wurde Louis Lévy, der auch französischer Repräsentant bei der Socialist Vanguard Group war.[1][2][3] Durch Philippe Barrès, den Sohn von Maurice Barrès, des 1923 verstorbenen Chefredakteurs von Le Matin und Paris-Soir, stieß im selben Jahr auch Stéphane Roussel zu der Gruppe und wurde in der Folge für France journalistisch tätig.[4] Roussel, erste Auslandskorrespondentin Frankreichs, war von 1934 bis 1938 Leiterin des Korrespondentenbüros von Le Matin in der deutschen Reichshauptstadt gewesen. Anlässlich eines privaten Aufenthalts in London war sie am 1. September 1939 vom Kriegsbeginn überrascht worden.[5] Die von der britischen Regierung finanziell unterstützte Zeitung war sozialistisch orientiert und nahm eine anti-gaullistische Haltung ein. France erschien bis zur Befreiung von Paris in der zweiten Augusthälfte 1944.
Die lateinische Sprache (lateinisch lingua Latina), kurz Latein, ist eine indogermanische Sprache, die ursprünglich von den Latinern, den Bewohnern von Latium mit Rom als Zentrum, gesprochen wurde. Die frühesten Zeugnisse reichen bis ins 5. oder 6. vorchristliche Jahrhundert zurück (Frühlatein), ab dem 3. vorchristlichen Jahrhundert liegen längere Texte vor (Altlatein), ihre volle Ausformung in der Gestalt des heute vor allem bekannten und gelehrten klassischen Lateins erreichte die (Schrift-)Sprache im ersten vorchristlichen Jahrhundert.

Latein war Amtssprache des Römischen Reichs und wurde so zur dominierenden Verkehrssprache im westlichen Mittelmeerraum. Während sich aus der gesprochenen Umgangssprache, dem sogenannten Vulgärlatein, im Frühmittelalter die romanischen Sprachen entwickelten, blieb das Latein der römischen Schriftsteller auch als tote Sprache bis in die Neuzeit die führende Sprache der Literatur, Wissenschaft, Politik und Kirche. Gelehrte wie Thomas von Aquin, Petrarca, Erasmus, Kopernikus, Descartes oder Newton haben Werke auf Latein verfasst. Bis ins 19. Jahrhundert wurden die Vorlesungen an den Universitäten in ganz Europa auf Latein gehalten; Dissertationen wurden, teils bis ins frühe 20. Jahrhundert, meist auf Latein verfasst. In Polen, Ungarn und im Heiligen Römischen Reich war Latein bis dahin Amtssprache. In Tausenden von Lehn- und Fremdwörtern sowie Redewendungen ist Latein heute auch in nichtromanischen Sprachen wie Deutsch oder Englisch präsent. Bei der Bildung neuer Fachbegriffe wird immer wieder auf Latein zurückgegriffen.

Wegen seiner enormen Bedeutung für die sprachliche und kulturelle Entwicklung Europas wird Latein vor allem in Deutschland,[1] Österreich und der Schweiz an vielen Schulen und Universitäten gelehrt. Für manche Studiengänge sind Lateinkenntnisse oder das Latinum erforderlich. Ähnlich stellt sich die Situation im Vereinigten Königreich dar, wo Latein bereits in der Primarstufe unterrichtet wird.
Ein Compiler (auch Kompilierer; von englisch compile ‚zusammentragen‘ bzw. lateinisch compilare ‚aufhäufen‘) ist ein Computerprogramm, das Quellcodes einer bestimmten Programmiersprache in eine Form übersetzt, die von einem Computer (direkter) ausgeführt werden kann. Daraus entsteht ein mehr oder weniger direkt ausführbares Programm. Davon zu unterscheiden sind Interpreter, etwa für frühe Versionen von BASIC, die keinen Maschinencode erzeugen.

Teils wird zwischen den Begriffen Übersetzer und Compiler unterschieden. Ein Übersetzer übersetzt ein Programm aus einer formalen Quellsprache in ein semantisches Äquivalent in einer formalen Zielsprache. Compiler sind spezielle Übersetzer, die Programmcode aus problemorientierten Programmiersprachen, sogenannten Hochsprachen, in ausführbaren Maschinencode einer bestimmten Architektur oder einen Zwischencode (Bytecode, p-Code oder .NET-Code) überführen. Diese Trennung zwischen den Begriffen Übersetzer und Compiler wird nicht in allen Fällen vorgenommen.

Der Vorgang der Übersetzung wird auch als Kompilierung oder Umwandlung (bzw. mit dem entsprechenden Verb) bezeichnet. Das Gegenteil, also die Rückübersetzung von Maschinensprache in Quelltext einer bestimmten Programmiersprache, wird Dekompilierung und entsprechende Programme Decompiler genannt.
Künstliche Intelligenz (KI), auch artifizielle Intelligenz (AI bzw. A. I.), englisch artificial intelligence (AI bzw. A. I.) ist ein Teilgebiet der Informatik, das sich mit der Automatisierung intelligenten Verhaltens und dem maschinellen Lernen befasst. Der Begriff ist schwer definierbar, da es bereits an einer genauen Definition von „Intelligenz“ mangelt. Dennoch wird er in Forschung und Entwicklung verwendet.

Meist bezeichnet künstliche Intelligenz den Versuch, bestimmte Entscheidungsstrukturen des Menschen nachzubilden, indem z. B. ein Computer so gebaut und programmiert wird, dass er relativ eigenständig Probleme bearbeiten kann. Oftmals wird damit aber auch eine nachgeahmte Intelligenz bezeichnet, wobei durch meist einfache Algorithmen ein „intelligentes Verhalten“ simuliert werden soll, etwa bei Computergegnern in Computerspielen.
Starke KI wären Computersysteme, die auf Augenhöhe mit Menschen die Arbeit zur Erledigung schwieriger Aufgaben übernehmen können. Demgegenüber geht es bei schwacher KI darum, konkrete Anwendungsprobleme zu meistern. Das menschliche Denken und technische Anwendungen sollen hier in Einzelbereichen unterstützt werden.[1] Die Fähigkeit zu lernen ist eine Hauptanforderung an KI-Systeme und muss ein integraler Bestandteil sein, der nicht erst nachträglich hinzugefügt werden darf. Ein zweites Hauptkriterium ist die Fähigkeit eines KI-Systems, mit Unsicherheit und probabilistischen Informationen umzugehen.[2] Insbesondere sind solche Anwendungen von Interesse, zu deren Lösung nach allgemeinem Verständnis eine Form von „Intelligenz“ notwendig zu sein scheint. Letztlich geht es der schwachen KI somit um die Simulation intelligenten Verhaltens mit Mitteln der Mathematik und der Informatik, es geht ihr nicht um Schaffung von Bewusstsein oder um ein tieferes Verständnis von Intelligenz. Während die Schaffung starker KI an ihrer philosophischen Fragestellung bis heute scheiterte, sind auf der Seite der schwachen KI in den letzten Jahren bedeutende Fortschritte erzielt worden.

Ein starkes KI-System muss nicht viele Gemeinsamkeiten mit dem Menschen haben. Es wird wahrscheinlich eine andersartige kognitive Architektur aufweisen und in seinen Entwicklungsstadien ebenfalls nicht mit den evolutionären kognitiven Stadien des menschlichen Denkens vergleichbar sein (Evolution des Denkens). Vor allem ist nicht anzunehmen, dass eine künstliche Intelligenz Gefühle wie Liebe, Hass, Angst oder Freude besitzt.[3]
Der Raspberry Pi (Aussprache in Britischem Englisch: ˈrɑːzb(ə)rɪ ˈpaɪ) ist ein Einplatinencomputer, der von der britischen Raspberry Pi Foundation entwickelt wurde. Der Rechner enthält ein Ein-Chip-System (SoC) von Broadcom mit einer Arm-CPU. Die Platine hat das Format einer Kreditkarte. Der Raspberry Pi kam Anfang 2012 auf den Markt; sein großer Markterfolg wird teils als Revival des bis dahin weitgehend bedeutungslos gewordenen Heimcomputers zum Programmieren und Experimentieren angesehen.[1] Der im Vergleich zu üblichen Personal Computern sehr einfach aufgebaute Rechner wurde von der Stiftung mit dem Ziel entwickelt, jungen Menschen den Erwerb von Programmier- und Hardware-Kenntnissen zu erleichtern. Entsprechend niedrig wurde der Verkaufspreis angesetzt, der je nach Modell etwa 5 bis 100 Euro beträgt.

Es ist der meistverkaufte britische Computer: Bis Mai 2021 wurden 40 Millionen Geräte verkauft.[2] Die Entwicklung des Raspberry Pi wurde mit mehreren Auszeichnungen und Ehrungen bedacht. Es existiert ein großes Zubehör- und Softwareangebot für zahlreiche Anwendungsbereiche. Verbreitet ist beispielsweise die Verwendung als Mediacenter, da der Rechner Videodaten im H.264-Codec mit voller HD-Auflösung (1080p) und HEVC-Videodaten mit 4K-Auflösung (4k60, seit Raspberry Pi 4) dekodieren und über die HDMI-Schnittstelle (1080p30 Raspberry Pi 3, 2× 4k60 ab Raspberry Pi 4) ausgeben kann. Als Betriebssystem kommen vor allem angepasste Linux-Distributionen mit grafischer Benutzeroberfläche zum Einsatz; für das neueste Modell existiert auch Windows 10 in einer speziellen Internet-of-Things-Version ohne grafische Benutzeroberfläche. Das Booten erfolgt gewöhnlich von einer wechselbaren SD-Karte. Bei der neueren Generation mit dem BCM2837 oder BCM2711 ist der Start auch von einem USB-Massenspeicher[3] oder Netzwerk[4] möglich. Eine native Schnittstelle für
Das Themengebiet der Robotik (auch Robotertechnik) befasst sich mit dem Versuch, das Konzept der Interaktion mit der physischen Welt auf Prinzipien der Informationstechnik sowie auf eine technisch machbare Kinetik zu reduzieren. Der Begriff des „Roboters“ beschreibt dabei eine Entität, welche diese beiden Konzepte in sich vereint, indem sie die Interaktion mit der physischen Welt auf der Basis von Sensoren, Aktoren und Informationsverarbeitung umsetzt. Kernbereich der Robotik ist die Entwicklung und Steuerung solcher Roboter. Sie umfasst Teilgebiete der Informatik (insbesondere von Künstlicher Intelligenz), der Elektrotechnik und des Maschinenbaus. Ziel der Robotik ist es, durch Programmierung ein gesteuertes Zusammenarbeiten von Roboter-Elektronik und Roboter-Mechanik herzustellen.

Den Begriff erfunden und geprägt hat der Science-Fiction-Autor Isaac Asimov, erstmals erwähnt wurde er in dessen Kurzgeschichte Runaround (dt. Herumtreiber) im März 1942 im Astounding-Magazin. Nach Asimovs Definition bezeichnet Robotik das Studium der Roboter oder auch der Maschinen.
Schon in der Antike wurden erste Versuche mit Automaten durchgeführt. Bekannt sind etwa automatische Theater und Musikmaschinen, erdacht durch Heron von Alexandria. Mit dem Niedergang der antiken Kulturen verschwanden temporär auch die wissenschaftlichen Erkenntnisse dieser Zeit (vgl. Bücherverluste in der Spätantike). Um 1205 verfasste Al-Dschazarī, Muslim-arabischer Ingenieur und Autor des 12. Jahrhunderts, sein Werk über mechanische Apparaturen, die Kitāb fī maʿrifat al-Hiyal al-handasīya „Buch des Wissens von sinnreichen mechanischen Vorrichtungen“, das auch als „Automata“ im westlichen Kulturbereich bekannt wurde. In diesem Werk bekundet er, dass er es für das Reich der Ortoqiden geschrieben habe. Er erstellte frühe humanoide Automaten, und einen Band über programmierbare (interpretierbar als Roboter, Händewasch-Automat, Automatisierte Verschiebung von Pfauen). Leonardo da Vinci soll von den klassischen Automaten von Al-Dschazarī beeinflusst worden sein. So sind von ihm Aufzeichnungen und Skizzen aus dem 15. Jahrhundert bekannt, die als Pläne für Androiden interpretiert werden können. Der technische Kenntnisstand reichte allerdings nicht aus, um derartige Pläne zu realisieren. Um 1740 konstruierte und erbaute Jacques de Vaucanson einen flötenspielenden Automaten, eine automatische Ente sowie den ersten programmierbaren vollautomatischen Webstuhl. In der Literatur wird letzterer Verdienst oft auch Joseph-Marie Jacquard 1805 zugeschrieben.

Ende des 19. Jahrhunderts wurden der Robotik zurechenbar Anstrengungen im Militärwesen unternommen (fernbedienbare Boote, Torpedosteuerungen). Der Schriftsteller Jules Verne schrieb eine Geschichte über eine Menschmaschine. 1920 führte der Schriftsteller Karel Čapek den Begriff Roboter für einen Androiden ein. Nach Ende des Zweiten Weltkrieges erfuhr der Bereich der Robotik rasante Fortschritte. Ausschlaggebend dafür waren die Erfindung des Transistors 1947 in den Bell Laboratories, integrierte Schaltkreise und in weiterer Folge die Entwicklung leistungsstarker und platzsparender Computer.

Ab etwa 1955 kamen erste NC-Maschinen auf den Markt (Geräte zur Steuerung von Maschinen) und 1954 meldet George Devol in den USA ein Patent für einen programmierbaren Manipulator an. Dieses Datum gilt als Geburtsstunde für die Entwicklung von Industrierobotern. Devol war auch Mitbegründer der Firma Unimation, die 1960 den ersten hydraulisch betriebenen Industrieroboter vorstellte. 1968 wird am MIT der erste mobile Roboter entwickelt.

In Deutschland wurde die Robotertechnik erst ab Anfang der 1970er Jahre produktiv eingesetzt.

Um 1970 wurde auch der erste autonome mobile Roboter Shakey (der Zittrige) am Stanford Research Institute entwickelt.

Im Jahr 1973 wurde an der Waseda-Universität Tokio die Entwicklung des humanoiden Roboters Wabot 1 gestartet. Im selben Jahr baute der deutsche Robotikpionier KUKA den weltweit ersten Industrieroboter mit sechs elektromechanisch angetriebenen Achsen, bekannt als FAMULUS.[1] Ein Jahr später (1974) stellte die schwedische ASEA ihren vollständig elektrisch angetriebene Roboter (IRb6) vor.

Im Jahr 1986 startete Honda das Humanoid Robot Research and Development Program. Ergebnis waren die humanoiden Roboterversionen P1 bis P3. Eine Weiterentwicklung stellte Honda 2004 in Form des humanoiden Roboter ASIMO vor.

1997 landete der erste mobile Roboter auf dem Mars (Sojourner).

Auch die Spielzeugindustrie hat sich der Robotik nicht verschlossen. Beispiele für derartige Erzeugnisse sind Lego Mindstorms, iPitara, Robonova oder der Roboterhund Aibo der Firma Sony.
Als Electrical Computer Aided Engineering (oft auch nur ECAE oder E-CAE) bezeichnet man die Computerunterstützung von Arbeitsprozessen in der Elektrotechnik. Sie bildet eine Spezialisierung des computer-aided engineering auf elektrotechnische Vorgänge.[1]
Science (englisch für Natur-, Sozial- und Formalwissenschaft) ist die Fachzeitschrift der American Association for the Advancement of Science (AAAS, englisch für Amerikanische Gesellschaft zur Förderung der Naturwissenschaften) und gilt neben Nature als die weltweit wichtigste ihrer Art. Die wöchentlich veröffentlichte Zeitschrift arbeitet nach dem Prinzip des Peer-Review und hat ungefähr 130.000 Abonnenten. Da zum einen unter diesen viele Institutionen wie Universitäten sind und die Zeitschrift andererseits über eine Online-Ausgabe verfügt, wird die tatsächliche Zahl der Leser auf etwa 1 Million geschätzt.
Das Entity-Relationship-Modell – kurz ER-Modell oder ERM; deutsch so viel wie: Modell (zur Darstellung) von Dingen, Gegenständen, Objekten (= ‚entities‘) und der Beziehungen/Zusammenhänge zwischen diesen (= ‚relationship‘) – dient dazu, im Rahmen der semantischen Datenmodellierung den in einem gegebenen Kontext (z. B. einem Projekt zur Erstellung eines Informationssystems) relevanten Ausschnitt der realen Welt zu bestimmen und darzustellen. Das ER-Modell besteht im Wesentlichen aus einer Grafik (ER-Diagramm, Abk. ERD) sowie einer Beschreibung der darin verwendeten Elemente.

Ein ER-Modell dient sowohl in der konzeptionellen Phase der Anwendungsentwicklung der Verständigung zwischen Anwendern und Entwicklern (dabei wird nur das Was behandelt, d. h. fachlich-sachliche Gegebenheiten, nicht das Wie, z. B. die Technik) als auch in der Implementierungsphase als Grundlage für das Design der – meist relationalen – Datenbank.

Der Einsatz von ER-Modellen ist der De-facto-Standard für die Datenmodellierung, auch wenn es unterschiedliche grafische Darstellungsformen für Datenmodelle gibt.

Das ER-Modell wurde 1976 von Peter Chen in seiner Veröffentlichung The Entity-Relationship Model vorgestellt. Die Beschreibungsmittel für Generalisierung und Aggregation wurden 1977 von John M. Smith und Diane C. P. Smith eingeführt. Danach gab es mehrere Weiterentwicklungen, so Ende der 1980er Jahre durch Wong und Katz.