Le français est une langue indo-européenne de la famille des langues romanes dont les locuteurs sont appelés francophones.

Le français est parlé, en 2018, sur tous les continents par environ 300 millions de personnes5,2 : 235 millions l'emploient quotidiennement, et 90 millions3 en sont des locuteurs natifs. En 2018, 80 millions d'élèves et étudiants s'instruisent en français dans le monde6. Selon l'Organisation internationale de la francophonie, il pourrait y avoir 700 millions de francophones sur Terre en 20507.

Dans le monde, 29 États ont le français comme langue officielle. C'est une des six langues officielles ainsi qu'une des deux langues de travail de l'Organisation des Nations unies. Le français est langue officielle ou de travail de nombreuses organisations gouvernementales internationales parmi lesquelles l'Union postale universelle ou les trois autorités mondiales de régulation du système métrique, il est langue officielle ou de travail de nombreuses organisations gouvernementales régionales telles que l'Union africaine ou l’Union européenne et est aussi langue officielle ou de travail pour de nombreuses organisations non gouvernementales internationales comme le Comité international olympique ou le Mouvement international de la Croix-Rouge et du Croissant-Rouge.

L'histoire du français et des francophones est celle de la rencontre et de l'échange entre de nombreux peuples. Le français est une variété de la langue d'oïl, un groupe de langues romanes parlées originellement dans la partie septentrionale du domaine gallo-roman sur le territoire des actuelles Belgique et France et qui résultent de l'évolution, sous l'influence de langues germaniques, du latin populaire parlé en Gaule. En 1539, par l’ordonnance de Villers-Cotterêts, le français, langue maternelle des dynasties capétiennes, devient une langue juridique et administrative en France. À la même période, il commence à se diffuser plus massivement hors d'Europe, d'abord en Amérique, puis ensuite en Afrique, en Asie et en Océanie, sous l'effet de l'expansion des empires coloniaux français puis belge. En 1794, par le décret révolutionnaire du 2 thermidor an II et malgré avoir été sous l'Ancien Régime la langue des cours royales et princières européennes, des tsars de Russie aux rois d'Espagne, des princes d'Allemagne aux rois de Grande-Bretagne, le français devient la seule langue officielle de la Première République française8. Une des particularités du français se trouve dans le fait que son développement et sa codification ont été en partie l'œuvre de groupes intellectuels, comme la Pléiade, ou d'institutions, comme l'Académie française. Le français est ainsi souvent considéré comme une langue « académique ».

La langue française est un attribut culturel souverain pour de nombreux peuples et États comme en France où depuis 1992 « la langue de la République est le français » ou au Québec où depuis 1977 elle « permet au peuple québécois d’exprimer son identité ». Elle est également le principal véhicule des cultures francophones dans le monde et le moyen principal d'expression de leurs pensées. La langue, parfois surnommée « langue de Molière9», ne cesse de s'enrichir que ce soit de façon formelle, par des décrets par exemple, mais aussi de façon informelle.

Le français est la deuxième langue la plus souvent enseignée en tant que langue étrangère à travers le monde, y compris aux États-Unis7. Il est également la quatrième langue la plus utilisée sur internet après l'espagnol, le mandarin et l'anglais
Computer Sciences Corporation (CSC, NYSE : CSC [archive]) est une société de services en ingénierie informatique américaine.

L'entreprise a fusionné avec Hewlett Packard Enterprise (HPE) Entreprise Services, pour devenir un des leaders du secteur. La nouvelle entité, se nomme DXC Technology4.
Computer Sciences Corporation a été fondée le 16 avril 1959 par Roy Nutt et Fletcher Jones1 avec 100 dollars5. Dès le début, CSC se base sur la technologie naissante de l'ordinateur : Nutt gère les aspects techniques et Jones la partie commerciale5. Le premier client est Honeywell en 19591 et en mai 1961, un contrat est signé avec le Jet Propulsion Laboratory de la National Aeronautics and Space Administration1. Dès avril 1962, l'entreprise enregistre un chiffre d'affaires annuel de plus d'un million de dollars1.

La compagnie réussit rapidement et devient, en 1968, la première compagnie de logiciels informatiques cotée en bourse.

Au deuxième semestre 2015, CSC scinde ses activités en deux : d'une part les activités qu'elle réalise avec l'administration américaine, et de l'autre les projets qu'elle mène auprès des entreprises et du secteur public au niveau mondial.6 Les activités destinées aux agences gouvernementales sont fusionnées avec SRA International, pour créer un nouvel ensemble appelé CSRA [archive] qui regroupe 19 000 salariés pour un chiffre d'affaires de 5,5 milliards de dollars7.

En mai 2016, Hewlett Packard Enterprise fusionne ses activités de services aux entreprises avec CSC, dans une transaction d'une valeur estimée à 8,5 milliards de dollars. Ces activités fusionnées représentent 37 % des activités de HP Enterprise pour un chiffre d'affaires de 4,7 milliards de dollars8.
La page Computer science a été trouvée.

Informatique (redirection depuis Computer science)
Computer science is no more about computers than astronomy is about telescopes — Hal Abelson. « La science informatique n'est pas plus la science des
La technologie est l'étude des outils et des techniques. Le terme désigne les observations sur l'état de l'art aux diverses périodes historiques, en matière d'outils et de savoir-faire. Il comprend l'art, l'artisanat, les métiers, les sciences appliquées et éventuellement les connaissances.

Par extension et abusivement, le mot désigne les systèmes ou méthodes d'organisation qui permettent les diverses technologies, ainsi que tous les domaines d'étude et les produits qui en résultent.
C'est semble-t-il un professeur de Harvard, Jacob Bigelow, qui aurait pour la première fois systématisé l'usage du mot technology en anglais dans son ouvrage Elements of technology' (1829)2. Botaniste et professeur à la chaire Rumford de Harvard consacrée à « l'application de la science aux arts utiles » (useful arts). Appelant à une véritable « fusion » entre les arts et la science, il réfute les savoirs fondamentaux qui ne s’articulent pas avec une pratique concrète et parallèlement les techniques (les arts dans les mots de l'époque) qui s’inscrivent dans une tradition sans le recours systématique au savoir scientifique. En appelant à une sectorialisation accrue des savoirs scientifiques et une répartition scientifique des tâches dans le domaine du travail, il va fournir à la société capitaliste américaine bientôt en expansion un véritable modèle d’éducation. C'est d'ailleurs sur les recommandations du professeur de Harvard que le MIT (Massachusetts Institute of Technology) empruntera son nom[réf. nécessaire], en lieu du « School of Industrial Science » comme prévu dans le projet du fondateur, mais aussi, de nombreuses orientations pédagogiques qui en feront un des centres de recherches « technologiques » les plus performants au monde (dans le domaine de la communication, de l'informatique et aujourd'hui de la robotique et de l'intelligence artificielle).

Le mot « technology » ne désignait pas pour Bigelow simplement les « arts utiles » mais suggérait en fait la convergence à restaurer à l’aube de la révolution industrielle entre les arts (tekhnê) et la science (logos) : une convergence compromise alors par l'angoisse naissante d'une impossible articulation des savoirs scientifiques se fragmentant avec leur diversification, et des arts nécessairement enfermés dans une tradition (ce que les membres du comité des arts et sciences américain nommaient « une routine empirique »). C'est ainsi que les premiers usages du terme dans le sens qu'en donna Bigelow précédèrent les bouleversements techniques du xixe siècle, et que l'usage du terme se répandit pendant la révolution industrielle.

Bigelow s'inscrit largement dans le sillage du « millénarisme technologique » qui anime avec ferveur l'enthousiasme scientifique et technique des nations occidentales (pour l'historien David Noble, il faut remonter au moine bénédictin Érigène promoteur d'un salut grâce aux « arts mécaniques »)3. Millénarisme séculier qui renvoie plus ou moins à l'idée d'un paradis sur terre qui s'incarne désormais dans le progrès technique (idée dont la diffusion est largement redevable aux philosophies progressistes de l'histoire européenne qui émergent au siècle des Lumières). L'une des influences majeures de cette téléologie du progrès technique fut sans aucun doute Francis Bacon : le chancelier d'Angleterre qui a initié la philosophie expérimentale, philosophie inductive qui marque une rupture fondamentale avec les approches scolastiques médiévales de la science (pour qui la nature s'appréhende par le prisme des dogmes de l'Église : la méthode « aprioriste »). Bacon était un fervent millénariste profondément imprégné de la rationalité puritaine (il restera anglican : fonctions obligent...).
Les premiers représentants du genre Homo sont le résultat d'une évolution à partir d'hominidés qui étaient déjà bipèdes5, avec une masse cérébrale d'approximativement un tiers de celle de l'homme moderne6. Les outils ont relativement peu évolué durant la plus grande partie de l'histoire humaine. Cependant, il y a environ 50 000 ans un ensemble complexe de comportements et d'utilisations d'outils a émergé. Certains archéologues y voient un lien avec l'émergence du langage structuré7.

Les ancêtres des hommes modernes ont utilisé des outils en pierre bien avant l'émergence d'Homo sapiens il y a 200 000 ans8. Les plus anciens outils de pierre connus, regroupés sous le nom de Pré-Oldowayen ou d'Oldowayen, datent d'il y a 2,3 millions d'années9. Des traces interprétées par leurs inventeurs comme des traces d'utilisation d'outils ont été observées sur des ossements découverts en Éthiopie dans la Vallée du Grand Rift. Elles datent d'il y a 2,5 millions d'années10 voire de 3,4 millions d'années11,12 Ces premières utilisations de la pierre marquent le début du Paléolithique, qui s'achève avec le développement de l'agriculture il y a environ 12 000 ans.

Pour fabriquer les plus simples outils en pierre, un bloc de roche dure aux propriétés mécaniques particulières, comme le silex, devait être frappé avec un percuteur également en pierre de façon à en détacher un éclat. Cette action produit un bord tranchant à la fois sur le bloc taillé et sur l'éclat qui en a été détaché, tous deux pouvant être utilisés comme outils. Les formes les plus simples sont le galet taillé et l'éclat, qui peut être transformé en racloir. Avec ces outils, les premiers humains, chasseurs-cueilleurs, ont pu exécuter différentes tâches, dont la découpe de la viande, la fracture des os pour accéder à la moelle osseuse, la coupe du bois, l'ouverture des noix, le dépouillement des carcasses animales pour récupérer la peau, et, par la suite, la fabrication d'autres outils avec des matériaux plus tendres comme l'os et le bois13. Les premiers outils en pierre sont relativement peu élaborés techniquement mais impliquent la maîtrise d'un nombre important de paramètres (choix de la matière première, choix du percuteur, intensité du coup, angle de percussion, etc.) hors de portée de tout espèce animale à l'exception de l'homme14. À l'Acheuléen, il y a 1,65 million d'années, de nouvelles méthodes de taille de la pierre apparaissent (façonnage), se traduisant par la production d'outils plus complexes comme le biface ou le hachereau. Il y a 300 000 ans, le Paléolithique moyen est caractérisé par la généralisation du débitage Levallois, permettant la production de séries d'éclats de forme prédéterminée aux dépens d'un même nucléus. Au Paléolithique supérieur, il y a environ 35 000 ans, le débitage de lames se généralise et permet la production de nouveaux outils (burin, grattoirs, etc.). La technique de la retouche par pression (attestée dès le Middle Stone Age en Afrique du Sud15) est utilisée pour retoucher certaines pointes de projectiles telles que les feuilles de laurier ou les pointes à cran16.

La découverte et l'exploitation du feu est un véritable tournant de l'évolution technologique du genre humain17. La date exacte de sa découverte est inconnue. La présence d'os d'animaux brûlés dans la région du Cradle of Humankind, à une cinquantaine de kilomètres à l'ouest de Johannesburg, suggère que la domestication du feu est apparue avant 1 million d'années avant le présent18. La communauté scientifique est quasiment unanime pour considérer que Homo erectus a contrôlé le feu aux alentours de 500 000 à 400 000 avant le présent19,20. Le feu, alimenté avec du bois, mais également avec du charbon, permettait aux premiers hommes de cuire leur nourriture et d'en améliorer la digestibilité, la conservabilité et la valeur nutritive en diversifiant la variété des préparations possibles21.
Un ordinateur est un système de traitement de l'information programmable tel que défini par Alan Turing et qui fonctionne par la lecture séquentielle d'un ensemble d'instructions, organisées en programmes, qui lui font exécuter des opérations logiques et arithmétiques. Sa structure physique actuelle fait que toutes les opérations reposent sur la logique binaire et sur des nombres formés à partir de chiffres binaires. Dès sa mise sous tension, un ordinateur exécute, l'une après l'autre, des instructions qui lui font lire, manipuler, puis réécrire un ensemble de données déterminées par une mémoire morte d'amorçage. Des tests et des sauts conditionnels permettent de passer à l'instruction suivante et donc d'agir différemment en fonction des données ou des nécessités du moment ou de l'environnement.

Les données à manipuler sont acquises soit par la lecture de mémoires, soit en provenance de périphériques internes ou externes (déplacement d'une souris, touche appuyée sur un clavier, déplacement d'un stylet sur une tablette, température et autres mesures physiques…). Une fois utilisés, ou manipulés, les résultats sont écrits soit dans des mémoires, soit dans des composants qui peuvent transformer une valeur binaire en une action physique (écriture sur une imprimante ou sur un moniteur, accélération ou freinage d'un véhicule, changement de température d'un four…). L'ordinateur peut aussi répondre à des interruptions qui lui permettent d’exécuter des programmes de réponses spécifiques à chacune, puis de reprendre l’exécution séquentielle du programme interrompu.

De 1834 à 1837, Charles Babbage conçoit une machine à calculer programmable en associant un des descendants de la Pascaline (première machine à calculer mécanique inventée par Blaise Pascal) avec des instructions écrites sur le même type de cartes perforées que celles inventées par Joseph Marie Jacquard pour ses métiers à tisser1. C'est durant cette période qu'il imagine la plupart des caractéristiques de l'ordinateur moderne2. Babbage passe le reste de sa vie à essayer de construire sa machine analytique, mais sans succès. Nombre de personnes essayent de développer cette machine3, mais c'est cent ans plus tard, en 1937, qu'IBM inaugure l'ère de l'informatique en commençant le développement de l'ASCC/Mark I, une machine construite sur l'architecture de Babbage qui, une fois réalisée, est considérée comme l'achèvement de son rêve4.

La technique actuelle des ordinateurs date du milieu du xxe siècle. Les ordinateurs peuvent être classés selon plusieurs critères tels que le domaine d'application, la taille ou l'architecture.
Le mot « ordinateur » fut introduit par IBM France en 19555,6 après que François Girard, alors responsable du service publicité de l'entreprise, eut l'idée de consulter son ancien professeur de lettres à Paris, Jacques Perret. Avec Christian de Waldner, alors président d'IBM France, ils demandèrent au professeur Perret, de suggérer un « nom français pour sa nouvelle machine électronique destinée au traitement de l'information (IBM 650), en évitant d'utiliser la traduction littérale du mot anglais computer (« calculateur » ou « calculatrice »), qui était à cette époque plutôt réservé aux machines scientifiques »7.

En 1911, une description de la machine analytique de Babbage utilisait le mot ordonnateur pour en décrire son organe moteur: « Pour aller prendre et reporter les nombres… et pour les soumettre à l’opération demandée, il faut qu'il y ait dans la machine un organe spécial et variable : c'est l'ordonnateur. Cet ordonnateur est constitué simplement par des feuilles de carton ajourées, analogues à celle des métiers Jacquard… »8.

Le professeur proposa un mot composé centré autour d'ordonnateur : celui qui met en ordre9 et qui avait aussi la notion d'ordre ecclésiastique dans l'église catholique (ordinant)10. Il suggéra plus précisément « ordinatrice électronique », le féminin ayant pu permettre, selon lui, de mieux distinguer l'usage religieux de l'usage comptable du mot11.

« IBM France retint le mot ordinateur et chercha au début à protéger ce nom comme une marque. Mais le mot fut facilement et rapidement adopté par les utilisateurs et IBM France décida au bout de quelques mois de le laisser dans le domaine public. »7
Sans une définition stricte il est impossible d'identifier la machine qui devint le premier ordinateur, mais il faut remarquer certaines des étapes fondamentales qui vont du développement du concept de la machine à calculer programmable par Charles Babbage en 1837 au premier développement de l'ère de l'informatique cent ans plus tard.

En 1834, Charles Babbage commence à développer une machine à calculer programmable, sa machine analytique. Il pense la programmer grâce à un cylindre à picots comme dans les automates de Vaucanson, mais, deux ans plus tard, il remplace ce cylindre par la lecture de cartes Jacquard, et ainsi crée une machine à calculer infiniment programmable15.

En 1843, Ada Lovelace écrit le premier programme informatique pour calculer les nombres de Bernoulli, pour la machine analytique qui ne sera jamais construite.

Henry Babbage construit une version extrêmement simplifiée de l'unité centrale de la « machine analytique » de son père et l'utilise en 1906, pour calculer et imprimer automatiquement les quarante premiers multiples du nombre Pi avec une précision de vingt-neuf décimales16, démontrant sans ambiguïté que le principe de la machine analytique était viable et réalisable. En 1886, sa plus grande contribution fut de donner un ensemble mécanique de démonstration d'une des machines de son père à l'université Harvard17. C'est cinquante ans plus tard, après avoir entendu la présentation de Howard Aiken sur son super calculateur, qu'un technicien de Harvard, Carmello Lanza, lui fit savoir qu'une machine similaire avait déjà été développée et qu'il lui montra l'ensemble mécanique de démonstration donné par Henry Babbage qui se trouvait dans un des greniers de l'université ; c'est ainsi qu'il découvrit les travaux de Babbage et qu'il les incorpora dans la machine qu'il présenta à IBM en 193718. C'était la troisième fois qu'il essayait de trouver un sponsor pour le développement de sa machine car son projet avait déjà été rejeté deux fois avant l'intégration des travaux de Babbage dans l'architecture de sa machine (une fois par la Monroe Calculating Company19 et une fois par l'université Harvard18).

Leonardo Torres Quevedo remplaça toutes les fonctions mécaniques de Babbage par des fonctions électromécaniques (addition, soustraction, multiplication et division mais aussi la lecture de cartes et les mémoires). En 1914 et en 1920, Il construisit deux machines analytiques, non programmable, extrêmement simplifiées20 mais qui montraient que des relais électromécaniques pouvaient être utilisés dans une machine à calculer qu'elle soit programmable ou non. Sa machine de 1914 avait une petite mémoire électromécanique et son arithmomètre de 1920, qu'il développa pour célébrer le centième anniversaire de l'invention de l'arithmomètre, était commandé par une machine à écrire qui était aussi utilisée pour imprimer ses résultats.

Percy Ludgate améliora et simplifia les fonctions mécaniques de Babbage mais ne construisit pas de machine. Et enfin, Louis Couffignal essaya au début des années 193021, de construire une machine analytique « purement mécanique, comme celle de Babbage, mais sensiblement plus simple », mais sans succès. C'est cent ans après la conceptualisation de l'ordinateur par Charles Babbage que le premier projet basé sur l'architecture de sa machine analytique aboutira. En effet, c'est en 1937 qu'Howard Aiken présenta à IBM un projet de machine à calculer programmable qui sera le premier projet qui finira par une machine qui puisse être, et qui sera utilisée, et dont les caractéristiques en font presque un ordinateur moderne. Et donc, bien que le premier ordinateur ne sera jamais déterminé à l’unanimité, le début de l'ère de l'informatique moderne peut être considéré comme la présentation d'Aiken à IBM, en 1937, qui aboutira par l'ASCC.
Artificial Intelligence est une série de huit albums parus sur le label britannique Warp Records entre 1992 et 1994, visant à mettre en valeur les possibilités de la musique électronique. Elle rassemble des artistes et groupes qui ont par la suite été reconnus dans le milieu de la musique électronique, tels Alex Paterson1, Plaid, Richard D. James1, Richie Hawtin ou Autechre. Elle est également une contribution significative au genre ensuite dénommé intelligent dance music.
La musique électronique est un type de musique conçu dans les années 1950 avec des générateurs de signaux et de sons synthétiques. Avant de pouvoir être utilisée en temps réel, elle a été primitivement enregistrée sur bande magnétique, ce qui permettait aux compositeurs de manier aisément les sons, par exemple dans l'utilisation de boucles répétitives superposées. Ses précurseurs ont pu bénéficier de studios spécialement équipés ou faisaient partie d'institutions musicales pré-existantes. La musique pour bande de Pierre Schaeffer, également appelée musique concrète, se distingue de ce type de musique dans la mesure où sa conception n'est pas basée sur l'utilisation d'un matériau précis mais l'inversion de la démarche de composition qui part des sons et non de la structure. La particularité de la musique électronique de l'époque est de n'utiliser que des sons générés par des appareils électroniques.
L'apprentissage automatique1,2 (en anglais : machine learning, litt. « apprentissage machine1,2 »), apprentissage artificiel1 ou apprentissage statistique est un champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes.

L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.

Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement3 si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.
L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.

L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle17.

Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…
L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire).

Exemples :

un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace[réf. nécessaire] ;
la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine 18,19, et ceux utilisés pour la reconnaissance d'écriture ou OCR.
Apprentissage non supervisé
Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.
Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).
Cette méthode est souvent source de sérendipité.
ex. : Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.).
Apprentissage semi-supervisé
Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner.
ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.
Apprentissage partiellement supervisé
Probabiliste ou non, quand l'étiquetage des données est partiel20. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).:
Apprentissage par renforcement21
l'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage.
ex. : L'algorithme de Q-Learning22 est un exemple classique.
Apprentissage par transfert23
L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. La question qui se pose est : comment identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis comment transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s) ?
Le terme probabilité possède plusieurs sens : venu historiquement du latin probabilitas, il désigne l'opposé du concept de certitude ; il est également une évaluation du caractère probable d'un événement, c'est-à-dire qu'une valeur permet de représenter son degré de certitude ; récemment, la probabilité est devenue une science mathématique et est appelée théorie des probabilités ou plus simplement probabilités; enfin une doctrine porte également le nom de probabilisme.

La probabilité d'un événement est un nombre réel compris entre 0 et 1. Plus ce nombre est grand, plus le risque, ou la chance, que l'événement se produise est grand. L'étude scientifique des probabilités est relativement récente dans l'histoire des mathématiques. L'étude des probabilités a connu de nombreux développements depuis le xviiie siècle grâce à l'étude de l'aspect aléatoire et en partie imprévisible de certains phénomènes, en particulier les jeux de hasard. Ceux-ci ont conduit les mathématiciens à développer une théorie qui a ensuite eu des implications dans des domaines aussi variés que la météorologie, la finance ou la chimie.

Un matériel informatique (en anglais : hardware) est une pièce ou composant d'un appareil informatique. C'est la partie physique de l’informatique elle est appairée avec le logiciel (software ou firmware). Il y a des composants situées à l'intérieur de l'appareil qui sont indispensables à son fonctionnement et, d'autres secondaires disposées à l'extérieur (les périphériques).

Les pièces intérieures sont, la plupart du temps, montées sur des circuits imprimés. Les pièces ou composants sont construites par différents fabricants et interconnectées entre elles. Le respect des normes et d'un cahier des charges très précis, par les différents fabricants permet le fonctionnement de l'ensemble.

Les pièces servent soit à recevoir des informations, à les envoyer, les échanger, les stocker ou les traiter. Toutes les opérations sont effectuées conformément aux instructions contenues dans les logiciels et aux manipulations des périphériques de l'interface homme-machine.
Un appareil informatique est un automate qui traite des informations conformément à des instructions préalablement enregistrées, selon le principe de la machine de Turing. Un micro-ordinateur est composé de :

Un processeur est un dispositif qui exécute des instructions de calcul sur des informations. Le processeur puise les informations et les instructions de traitements dans des mémoires ou des dispositifs de stockage d'informations (internes ou externes). Les résultats des traitements sont eux aussi placés dans les mémoires ;
Des dispositifs d'entrée permettent à un humain de commander l'appareil informatique et d'y introduire des informations (clavier, souris, clé USB, CD, etc.). D'autres périphériques (internes ou externes) permettent de connaître différents paramètres (position géographique, température, etc.) ;
Des dispositifs de sortie servent à extraire les informations de l'appareil informatique, et les présenter sous une forme utilisable par un humain (écran, haut parleur, etc.) ou par un périphérique tel qu'un DVD, une clé USB, etc. ;
Des dispositifs de télécommunication permettent l'échange d'informations entre différents appareils informatiques grâce au réseau informatique (bluetooth, WIFI, Ethernet, etc.). Les câbles filaire assurent une connexion plus fiable et plus précise, que les transmission par ondes radioélectriques, quel que soit le périphérique, le temps et la disposition des locaux.
L'intérieur du boîtier d'un appareil informatique contient un ou plusieurs circuits imprimés sur lesquels sont soudés des composants électroniques et des connecteurs. La carte mère est le circuit imprimé central, sur lequel sont connectés tous les autres équipements. Les périphériques sont par définition les équipements situés à l'extérieur du boîtier. Ces équipements peuvent être connectés par des câbles, qui sont souvent des bus informatiques ou Ethernet mais de plus en plus souvent par des liaisons radios (WIFI, Bluetooth, etc.)
Les lecteurs de cartes perforées (Herman Hollerith, 1890) sont les périphériques d'entrée qui équipaient les premiers ordinateurs. Ils ont été utilisés jusque dans les années 1980. Les cartes perforées servaient en même temps de mémoire de masse, de périphérique d'entrée et de périphérique de sortie.

Un télétype est un type d'appareil créé en 1925, semblable à une machine à écrire et utilisé pour introduire des textes dans un système informatique.

Un terminal est un groupe de périphériques d'entrée et de sortie (clavier, souris, écran), utilisés pour commander à distance un appareil informatique. Les terminaux étaient d'usage courant sur les ordinateurs jusque dans les années 1980notes 1.

Un émulateur de terminal est un logiciel placé dans un appareil informatique pour permettre à l'utilisateur de piloter un autre appareil comme s'il se trouvait aux commandes d'un terminal.
Un robot est un dispositif mécatronique (alliant mécanique, électronique et informatique) conçu pour accomplir automatiquement des tâches imitant ou reproduisant, dans un domaine précis, des actions humaines. La conception de ces systèmes est l'objet d'une discipline scientifique, branche de l'automatisme nommé robotique.

Le terme robot apparaît pour la première fois dans la pièce de théâtre (science-fiction) R. U. R. (Rossum's Universal Robots), écrite en 1920 par l'auteur Karel Čapek1. Le mot a été créé par son frère Josef à partir du mot tchèque « robota » qui signifie « travail, besogne, corvée ».

Les premiers robots industriels apparaissent, malgré leur coût élevé, au début des années 1970. Ils sont destinés à exécuter certaines tâches répétitives, éprouvantes ou toxiques pour un opérateur humain : peinture ou soudage des carrosseries automobiles. Aujourd'hui, l'évolution de l'électronique et de l'informatique permet de développer des robots plus précis, plus rapides ou avec une meilleure autonomie. Industriels, militaires ou spécialistes chirurgicaux rivalisent d'inventivité pour mettre au point des robots assistants les aidant dans la réalisation de tâches délicates ou dangereuses. Dans le même temps apparaissent des robots à usages domestiques : aspirateur, tondeuses, etc.

L'usage du terme « robot » s'est galvaudé pour prendre des sens plus larges : automate distributeur, dispositif électro-mécanique de forme humaine ou animale, logiciel servant d'adversaire sur les plateformes de jeu, bot informatique.
Le terme robot est issu des langues slaves et formé à partir du radical rabot, rabota (работа en russe) qui signifie travail, corvée que l'on retrouve dans le mot Rab (раб), esclave en russe. Ce radical présent dans les autres langues slaves (ex. : travailleur = robotnik en polonais, работнік en biélorusse, pracovník en tchèque) provient de l'indo-européen *orbho- qui a également donné naissance au gotique arbais signifiant besoin, nécessité, lui-même source de l'allemand Arbeit, travail2.

Il fut initialement utilisé par l’écrivain tchécoslovaque Karel Čapek dans sa pièce de théâtre R. U. R. (Rossum's Universal Robots), écrite en 1920. Cette pièce fut jouée pour la première fois en public au Théâtre national à Prague le 25 janvier 19213. Bien que Karel Čapek soit souvent considéré comme l’inventeur du mot, il a lui-même désigné son frère Josef, peintre et écrivain, comme étant l’inventeur réel du terme4.

Ainsi certains assurent que le mot robot fut d’abord utilisé dans la courte pièce Opilec de Josef Čapek (The Drunkard), publiée dans la collection Lelio en 1917. Selon la Société des frères Čapek à Prague, ce serait néanmoins inexact. Le mot employé dans Opilec est automate, alors que c'est bien dans R.U.R. que le mot robot est apparu pour la première fois.

Alors que les « robots » de Karel Čapek étaient des humains organiques artificiels, le mot robot fut emprunté pour désigner des humains « mécaniques ». Le terme androïde peut signifier l’un ou l’autre, alors que le terme cyborg (« organisme cybernétique » ou « homme bionique ») désigne une créature faite de parties organiques et artificielles.

Quant au terme robotique, il fut introduit dans la littérature en 1942 par Isaac Asimov dans son livre Runaround. Il y énonce les « trois règles de la robotique » qui deviendront, par la suite, dans les œuvres de sciences fiction les Trois lois de la robotique.
Le mot article, du latin artus : « articulation », indique un élément cohérent d'une décomposition. Il possède les significations suivantes :

En grammaire, un article est un déterminant placé devant1 le nom, précisant le degré de définition de ce nom.
Un article est un texte traitant d'un sujet particulier dans une revue ou un ouvrage en comportant plusieurs éléments :
Un article de journal ou de magazine présente des faits d’actualité ou un reportage.
Dans une encyclopédie, l'article est une entrée qui comporte du texte et d'autres informations sur un sujet (par exemple, une biographie, la description d'un événement ou une étude succincte sur l'influence d'un courant artistique)
En production, un article est un objet géré dans la chaîne de production.
Dans le commerce, un article est un des biens vendus par un magasin.
En droit, l'article est la subdivision de base d'un texte législatif (loi, code, convention, etc.). Chacun des articles d'un texte porte un numéro permettant d'y faire référence.
On désigne en zoologie par article chacune des parties constituant une antenne, une patte, un tarse ou un palpe chez les animaux articulés comme les insectes, les crustacés, les araignées, les myriapodes; il désigne aussi les segments constituant le corps des arthropodes et du pédoncule chez les crinoïdes.
L’université Harvard (Harvard University), ou plus simplement Harvard, est une université privée américaine située à Cambridge, ville de l'agglomération de Boston, dans le Massachusetts. Fondée le 28 octobre 16361, c'est le plus ancien établissement d'enseignement supérieur des États-Unis2,3.

C'est une des institutions les plus prestigieuses au monde, en raison de son histoire, son influence, sa richesse et sa réputation4,5,6,7,8,9. Le taux d'admission est de seulement 4,7 % en 202010. Elle est considérée comme la meilleure université au monde par de nombreux classements, dont celui de Shanghaï.

Elle fait partie de l'Ivy League, regroupement informel de huit universités d'élite de la côte Est des États-Unis. Le corps enseignant est constitué de 2 497 professeurs, pour 6 715 étudiants de premier cycle (undergraduate, en anglais) et 12 424 étudiants de cycle supérieur (graduate en anglais). Harvard attire des étudiants du monde entier (132 nationalités représentées en 200411).

En mars 2020, 160 lauréats du Prix Nobel; 18 médaillés Fields; 14 lauréats du Prix Turing12; 10 récompensés aux Oscars; 48 Prix Pulitzer et 108 médailles olympiques (46 médailles d'or, 41 d'argent et 21 de bronze)13,14 sont affiliés à Harvard en tant qu'étudiants, professeurs ou chercheurs. 8 présidents américains, plus de 30 dirigeants mondiaux, 188 milliardaires vivants, 369 boursiers Rhodes et 252 boursiers Marshall15,16,17 sont notamment sortis des rangs de Harvard.
Harvard à l’époque coloniale[modifier | modifier le code]

Harvard au xviiie siècle.
Le vrai collège, tel que nous le connaissons aujourd'hui, est fondé en 163618, par un vote de l'assemblée générale de la colonie de la baie du Massachusetts (Massachusetts Bay Colony). En 1639, il est baptisé « Harvard » en hommage à John Harvard, de Charlestown, jeune pasteur puritain qui en 1638 légua sa bibliothèque et la moitié de ses biens à la jeune institution3. Ce legs constitue le fonds originel de la Bibliothèque de Harvard. À ses débuts, l'établissement ne compte que neuf étudiants et un professeur, Nathaniel Eaton3 ; l'enseignement est proche de celui qui était dispensé en Angleterre mais subit l'influence du puritanisme des premiers colons de la Nouvelle-Angleterre. Harvard forme alors de nombreux pasteurs. La première bourse d'étude est fondée en 16433. En 1766 a lieu ce qui est considéré comme la première grève étudiante du pays19.

Pendant la guerre d'indépendance américaine, les soldats américains sont hébergés dans le Massachusetts Hall20.

La dénomination d'« université », en revanche, ne date que de 1780.
Le latin (en latin : Lingua Latīna ou Latīna Lingua) est une langue italique de la famille des langues indo-européennes, parlée à l'origine par les Latins dans le Latium de la Rome antique. Bien qu'il soit souvent considéré comme une langue mortenote 1, sa connaissance, voire son usage, se sont maintenus à l'université et dans le clergé. De nombreuses écoles et universités continuent à l'enseigner1,note 2. Le latin est toujours utilisé pour la production de néologismes dans de nombreuses familles de langues. Le latin, ainsi que les langues romanes (dites parfois néo-latines), sont la seule branche des langues italiques à avoir survécu. Les autres branches sont attestées dans des documents datant de l'Italie préromaine, mais ont été assimilées durant la période républicaine ou au début de l'époque impériale.


Arc de Sisto le V à Rome, plaque et inscription en latin
Langue flexionnelle, elle comporte sept cas, deux nombres et trois genres. L'alphabet latin est dérivé des alphabets étrusque et grec. Enrichi de lettres supplémentaires et de signes diacritiques, il est utilisé aujourd'hui par de nombreuses langues vivantes et comportait à l'époque classique 23 lettres, dont 4 voyelles, 2 semi-voyelles et 17 consonnes.
Les langues italiques formaient, à côté des langues celtiques, germaniques et helléniques, une sous-famille « centum » des langues indo-européennes qui incluait le latin, parlé par la population du Latium en Italie centrale (les Latins), et d'autres parlers comme l'ombrien et l'osque, au voisinage immédiat d'une langue étrusque non indo-européenne mais dont le latin a subi l'influence culturelle. De nos jours, les langues italiques sont représentées par les langues romanes, issues du latin populaire (l'italien, le roumain/moldave, l'aroumain, le français, l'occitan, le francoprovençal, le catalan, l'espagnol, le portugais, le sarde, le ladin, le corse, etc., ainsi que des langues aujourd'hui éteintes, comme le dalmate ou le mozarabe).
La coopération économique est approfondie par l’Acte unique européen en 1986. En 1992, le traité de Maastricht prend la suite de l’Acte unique et institue une union politique qui prend le nom d’Union européenne et qui prévoit la création d'une union économique et monétaire dotée d’une monnaie unique : l’euro (€). Instituée en 1999, la zone euro compte dix-neuf États en 2017. De nouvelles réformes institutionnelles sont introduites en 1997 et en 2001. À la suite de l’échec d’un projet de constitution européenne après le refus par référendum des peuples français et néerlandais, les institutions sont à nouveau réformées en 2009 par le traité de Lisbonne pour y intégrer les mesures prévues par ce projet de constitution.

Depuis la formation de la CEE, le nombre d'États membres est passé de 6 à 27. Les membres fondateurs de la Communauté économique européenne, en 1957, sont l'AllemagneNote 4, la Belgique, la France, l'Italie, le Luxembourg et les Pays-Bas. Ils sont rejoints en 1973 par trois membres de l'Association européenne de libre-échange : le Danemark, l'Irlande et le Royaume-Uni. L'Union s'élargit vers le sud avec d'abord l'adhésion de la Grèce en 1981, puis celle de l'Espagne et du Portugal en 1986. Entre-temps, en 1985, le Groenland a décidé de se retirer en ratifiant le Traité sur le Groenland et a désormais le statut de pays et territoire d'outre-mer associé. Avec la fin de la Guerre froide, la partie orientale de l'Allemagne rejoint la Communauté économique européenne en 1990Note 5. L'Union européenne intègre en 1995 des États neutres : l'Autriche, la Finlande et la Suède. En 2004, dix nouveaux États, en majorité issus du bloc de l'Est, s'ajoutent aux quinze déjà membres : Chypre, l'Estonie, la Hongrie, la Lettonie, la Lituanie, Malte, la Pologne, la Slovaquie, la Slovénie et la Tchéquie. Deux États supplémentaires, la Bulgarie et la Roumanie, complètent en 2007 ce cinquième élargissement, La Croatie devient en 2013 le 28e membre de l'Union13. Enfin, en 2020, le Royaume-Uni quitte l'Union à la suite d'un référendum des citoyens britanniques.

En raison de sa « contribution à la promotion de la paix, la réconciliation, la démocratie et les droits de l'Homme en Europe »14, l'Union européenne a reçu, le 12 octobre 2012, le prix Nobel de la paix.
L'Union européenne (UE)Note 3 est une union politico-économique sui generis de vingt-sept États européens qui délèguent ou transmettent par traité l’exercice de certaines compétences à des organes communautaires5,6. Elle s'étend sur un territoire de 4,2 millions de kilomètres carrés7, est peuplée de plus de 446 millions d'habitants2 et est la deuxième puissance économique mondiale en termes de PIB nominal derrière les États-Unis et devant la Chine8,9,10,11. L’Union européenne est régie par le traité de Maastricht (TUE) et le traité de Rome (TFUE), dans leur version actuelle, depuis le 1er décembre 2009 et l'entrée en vigueur du traité de Lisbonne. Sa structure institutionnelle est en partie supranationale et en partie intergouvernementale : le Parlement européen est élu au suffrage universel direct, tandis que le Conseil européen et le Conseil de l'Union européenne (informellement le « Conseil des ministres ») sont composés de représentants des États membres. Le président de la Commission européenne est pour sa part élu par le Parlement sur proposition du Conseil européen. La Cour de justice de l'Union européenne est chargée de veiller à l'application du droit de l'Union européenne.
La déclaration du 9 mai 1950 de Robert Schuman, alors ministre français des Affaires étrangères, est considérée comme le texte fondateur de la construction européenne. Sous l’impulsion de personnalités politiques surnommées les « pères de l'Europe »12, comme Konrad Adenauer, Jean Monnet et Alcide De Gasperi, six États créent en 1951 la Communauté européenne du charbon et de l'acier. Après l’échec d'une Communauté européenne de défense en 1954, une Communauté économique européenne (CEE) est instaurée en 1957 par le traité de Rome.
L’Iran (en persan : ايران, IrânÉcouter), en forme longue la république islamique d'Iran (en persan : جمهوری اسلامی ايراﻥ, Jomhuriye Eslâmiye Irân ou JEIÉcouter), est un pays d'Asie de l'Ouest, historiquement appelé la Perse3. Bordé au nord par la mer Caspienne, au sud-est par le golfe d'Oman et au sud par le golfe Persique, l'Iran partage des frontières avec le Turkménistan au nord-est, l'Afghanistan à l'est, le Pakistan au sud-est, l'Irak à l'ouest et la Turquie, l'Arménie et l'Azerbaïdjan au nord-ouest. Le pays a une superficie de 1 648 195 km2.

L'Iran est un pays fortement diversifié tant sur le plan des grands ensembles naturels que de sa population et sa culture. Le relief de l'Iran est montagneux à l'ouest et au nord — les sommets sont les plus hauts d'Eurasie à l'ouest de l'Hindou Kouch-Himalaya — et à l'est, le plateau iranien s'insérant entre les deux massifs et les plaines étant circonscrites aux côtes de la mer Caspienne et du golfe Persique. À la rencontre des plaques eurasiatique, arabique et indienne, le pays est sujet aux séismes. Les aires à l'ouest et au nord, plus humides et couvertes de steppes et de forêts, rassemblent la plus grande partie de la population, l'est et le sud étant semi-désertiques et désertiques.

L'Iran est l'un des plus anciens berceaux civilisationnels du monde, ayant été habité par les Élamites dès le IVe millénaire avant notre ère. Unifié par les Mèdes, le territoire vint à constituer l'un des plus vastes empires à avoir jamais existé, s'étendant de l'est de l'Europe à la vallée de l'Indus sous le règne des Achéménides, ainsi que le plus important foyer du monothéisme zoroastrien pendant plus de mille ans. Conquis en 331 avant notre ère par Alexandre le Grand et placé sous la domination des rois séleucides, l'empire se rebella au siècle suivant sous l'impulsion des Parthes. Régnant à partir du iiie siècle de notre ère, les Sassanides érigèrent l'Empire perse au rang de grande puissance de l'Asie de l'Ouest pendant plus de quatre cents ans4. La conquête arabo-musulmane au viie siècle conduisit à l'islamisation de l'Iran, dont les contributions aux arts, aux sciences et à la philosophie au cours de l'Âge d'or de l'islam furent toutefois nombreuses. L'Iran fut gouverné au cours des deux siècles qui suivirent par des dynasties locales puis par les Turcs seldjoukides puis les Ilkhans mongols. La dynastie séfévide unifie à nouveau l'Iran au xve siècle et fait de l'islam chiite la religion officielle. Après avoir été une puissance majeure sous Nader Chah au xviiie siècle, l'Iran subit des pertes territoriales face à l'Empire russe. Au début du xxe siècle, la Révolution constitutionnelle persane aboutit à l'instauration d'un parlement. Un coup d'État est opéré par le Royaume-Uni et les États-Unis en 1953. La Révolution islamique en 1979 aboutit à l'établissement de l'actuel régime politique de l'Iran.

L'Iran compte 82 801 633 habitants1. La langue officielle est le persan et plusieurs minorités parlant azéri, kurde, lori, guilaki, baloutchi, mazandarani, kachkaï et arabe1 peuplent différentes villes des 31 provinces. La capitale est Téhéran. Le calendrier officiel est le calendrier persan. L'Iran est la 28e puissance économique mondiale selon le produit intérieur brut (PIB) nominal et la dix-huitième selon le PIB à parité de pouvoir d'achat (2015). Le PIB par habitant s’élève à 11 200 $US (2011)1. Membre de l'Organisation des pays exportateurs de pétrole (OPEP), c'est un important producteur de pétrole à l'échelle mondiale. Il dispose de la plus grande réserve de gaz naturel. La monnaie est le rial.
Le toponyme Iran, qui signifie « royaume des Aryens »5, d'usage natif depuis l'ère sassanide, est officiellement adopté le 21 mars 1935 pour l'usage international6. Auparavant, le pays était connu en Occident sous le nom de Perse. Les noms « Perse » et « Iran » sont souvent utilisés indifféremment dans le contexte culturel, bien que le terme « Iran » demeure utilisé officiellement dans le contexte politique7.

Le mot Iran a une racine aussi ancienne que les langues indo-européennes. Aussi bien mythologiquement qu'historiquement, c'est la base d'un mot à la fois complexe et commun, couvrant un espace étendu allant de l'Iran à l’Écosse8. Pendant la dynastie des Achéménides (-559 à -330), les Iraniens appelaient leurs territoires Parsa du nom de l'empire de Cyrus le Grand, de la tribu perse, qui se retrouve aujourd'hui sous la forme de Fars ou Pars, ville et province d’Iran. Cependant, la totalité de l’État était alors appelée Aryanam. Ce mot est apparenté au terme Aryen, qui signifie noble9. À l’époque parthe (-248 - 224), Aryanam a été modifié en Aryan pour évoluer vers Iranchahr et Iran à l’époque sassanide. Les Grecs appelaient les Perses du nom de Mèdes, les confondant avec un peuple que les Perses avaient soumis auparavant. Ils utilisaient les termes Aryana et Persis pour désigner la région aujourd’hui connue comme le plateau Iranien10. Le terme est passé au latin pour devenir Persia, puis en français Perse, terme encore utilisé dans les pays occidentaux. Le 21 mars 1935, Reza Shah Pahlavi publie un décret demandant à toutes les relations étrangères du pays de le désigner sous le nom d'Iran dans leur correspondance officielle, sans que le terme Perse tombe dans l'inusité10. En 1959, le gouvernement a annoncé que les deux noms (Perse et Iran) peuvent être officiellement utilisés de manière interchangeable3. En 1979 en Iran, la révolution iranienne proclame la République islamique d’Iran, désignation officielle actuelle. Les termes Perse et Iran sont toujours largement utilisés.
L’arabe (en arabe : العربية, al-ʿarabīyah4 /alʕaraˈbijja/Écouter) est une langue afro-asiatique de la famille des langues sémitiques. Avec un nombre de locuteurs estimé entre 315 421 3001 et 375 millions de personnes2 au sein du monde arabe et de la diaspora arabe, l'arabe est de loin la langue sémitique la plus parlée, bien avant l'amharique (seconde langue sémitique la plus parlée).

La langue arabe est originaire de la péninsule Arabique, où elle devint au viie siècle la langue du Coran et la langue liturgique de l'islam. L'expansion territoriale de l'Empire arabe au Moyen Âge a conduit à l'arabisation au moins partielle sur des périodes plus ou moins longues du Moyen-Orient, de l'Afrique du Nord et de certaines régions en Europe (péninsule Ibérique, Sicile, Crète, Chypre, territoires d'où elle a disparu, et Malte, où le maltais en constitue un prolongement particulier). Parlée d'abord par les Arabes, cette langue qui se déploie géographiquement sur plusieurs continents s'étend sociologiquement à des peuples non arabes, et est devenue aujourd'hui l'une des langues les plus parlées dans le monde. C'est la langue officielle de plus de vingt pays et de plusieurs organismes internationaux, dont l'une des six langues officielles de l’Organisation des Nations unies.

La langue arabe est marquée par une importante diglossie entre l'arabe littéral, langue véhiculaire surtout écrite, et l'arabe dialectal, langue vernaculaire surtout orale. L'arabe littéral comprend l'arabe classique (pré-coranique, coranique, et post-coranique) et l'arabe standard moderne. L'arabe dialectal comprend de nombreuses variétés régionales, pas toutes intelligibles entre elles.

Les vecteurs du rayonnement culturel de la langue arabe sont l'islam, la littérature de langue arabe et les médias audiovisuels contemporains dont la télévision et Internet. Un vecteur historique important de rayonnement fut l'emprunt lexical de termes arabes dans des langues étrangères, entre autres les langues romanes dont le français.

La prononciation de l'arabe comporte un nombre assez élevé de consonnes (28 en arabe littéral) et peu de voyelles (trois timbres et deux longueurs en littéral, souvent un peu plus en dialectal). L'arabe s'écrit au moyen de l'alphabet arabe.

Par sa grammaire, l'arabe est une langue accusative et flexionnelle qui fait un usage important de la flexion interne. La syntaxe suit dans la proposition l'ordre fondamental verbe-sujet-objet, et le déterminant suit le déterminé dans le groupe nominal.

Des sciences linguistiques complémentaires à l'étude de la grammaire sont la sémantique et la stylistique de l'arabe, ainsi que sa lexicographie qui étudie le vocabulaire et permet l'élaboration de dictionnaires.
America est un hebdomadaire catholique américain, fondé en 1909 à New York et dirigée par l’ordre des Jésuites.

Revue de société et d'opinion, America propose des informations sur l'actualité de l'Église catholique ainsi que sur l'incidence de ses débats internes ou prises de position dans la culture et la politique américaines.
Histoire[modifier | modifier le code]
La revue a été fondée à New York en 1909 sous l’impulsion du jésuite John J. Wynne1 et elle est toujours dirigée par les jésuites américains. Elle a une circulation de 45 000 lecteurs. Elle se décrit comme un magazine d'opinion et d'information qui renseigne la communauté des intellectuels catholiques ainsi que ceux qui sont intéressés par ce que pense le peuple de l'Église.

Orientations[modifier | modifier le code]
En raison de ses origines jésuites, America a la réputation d'être d'un catholicisme modéré ou libéral, ce qui lui a causé des ennuis avec la Curie romaine.

America n'hésite pas à publier articles et opinions sur des questions qui font débat dans l'Église catholique contemporaine, y compris la place des femmes dans l'Église catholique, le célibat sacerdotal, la place de la sexualité, le dialogue interreligieux, la nouvelle traduction anglaise des textes liturgiques, ou encore la déclaration Dominus Iesus considérée par certains comme un recul2. Pour ces raisons, la revue fait l'objet d'une surveillance par le Vatican.

Son rédacteur en chef, Thomas J. Reese, a été remplacé au printemps 2005 par Drew Christiansen, à l'instigation de la Congrégation pour la doctrine de la foi, organe de la curie veillant sur l'orthodoxie alors dirigé par Joseph Ratzinger3. Depuis 2012, le poste est occupé par Matt Malone.