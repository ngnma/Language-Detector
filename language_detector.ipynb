{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# languages detector ML project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ngn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from nltk.collections import defaultdict\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "#from hazm import *\n",
    "import pandas as pd\n",
    "tokenizer=WordPunctTokenizer()\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    \n",
    "    def __init__(self,name):\n",
    "        self.name=name\n",
    "    \n",
    "    def load_dataset(self,file):\n",
    "        #\"/Users/ngn/Desktop/english.txt\"\n",
    "        self.dataset=open(file,encoding=\"utf-8\").read()\n",
    "        return 'true'\n",
    "    \n",
    "    def seprate_train_test_set(self):\n",
    "        self.sen=sent_tokenize(self.dataset)\n",
    "        random.shuffle(self.sen)\n",
    "        cut=int((4/5)*len(self.sen))\n",
    "        self.train=self.sen[:cut]\n",
    "        self.test=self.sen[cut:]\n",
    "        return self.train,self.test\n",
    "    \n",
    "    def set_stop_word(self,stp):\n",
    "        self.stp=stp\n",
    "        \n",
    "        \n",
    "    def remove_1_2_chars(self):\n",
    "        for word in self.bag_of_word:\n",
    "            if len(word)<3:\n",
    "                self.bag_of_word.remove(word)  \n",
    "        \n",
    "        \n",
    "    def remove_special_chars(self,text):\n",
    "        chars=['0','1','2','3','4','5','6','7','8','9','\"',\"'\",'=','@','&','%','.',',',':','\\\\','$','^','<','>','!','?','{','}',';','\\n','\\t','(',')','[',']','/','*','+','#','\\u200c','\\ufeff','-','_','|','۱','۲','۳','۴','۵','۶','۷','۸','۹','۰','،']\n",
    "        for item in chars:\n",
    "            text=text.replace(item,\"\")\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self):\n",
    "        self.bag_of_word=[w for w in self.bag_of_word if w not in self.stp]\n",
    "    \n",
    "    def preproccessing(self):\n",
    "        tokenizer=WordPunctTokenizer()\n",
    "        text=''.join(self.train)\n",
    "        text=self.remove_special_chars(text)\n",
    "        self.bag_of_word=tokenizer.tokenize(text)\n",
    "        self.remove_stopwords()\n",
    "        self.remove_1_2_chars()\n",
    "        #few more proccess here\n",
    "        return self.bag_of_word,len(self.bag_of_word)\n",
    "\n",
    "    \n",
    "    def defualt_value(self,v):\n",
    "        return (1/(len(self.bag_of_word)+v))\n",
    "    \n",
    "    \n",
    "    def create_features_dict(self,defualt_value_0):\n",
    "        self.features_dict=defaultdict(defualt_value_0)#defualt_value_0 static in main benevis\n",
    "        for item in self.bag_of_word:\n",
    "            self.features_dict[item]+=1\n",
    "            \n",
    "    def probability_conditional_all(self,v):\n",
    "        # this for calculate p(key|english boodan)\n",
    "        for key,val in self.features_dict.items():\n",
    "            self.features_dict[key]=(val+1)/(len(self.bag_of_word)+v)\n",
    "            \n",
    "    def probability_conditional(self,feat,v):\n",
    "        # this for calculate p(feat|english boodan)\n",
    "        return (self.features_dict[feat]+1)/(len(self.bag_of_word)+v)\n",
    "    \n",
    "    def train_len(self):\n",
    "        return len(self.train)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def probability_class(train_lens):\n",
    "    p_class=[]\n",
    "    sl=sum(train_lens)\n",
    "    for item in train_lens:\n",
    "        p_class.append(int(item)/sl)\n",
    "    return p_class\n",
    "\n",
    "\n",
    "def test_case(string,languages,prob_classes):\n",
    "    tokens=tokenizer.tokenize(string)#comment this line\n",
    "    for feat in tokens:#tokens->string\n",
    "        for i in range(len(languages)):\n",
    "            prob_classes[i] *= languages[i].probability_conditional(feat,len(features))\n",
    "            if prob_classes.count(0.0)==len(languages)-1:\n",
    "                return prob_classes\n",
    "    return prob_classes\n",
    "\n",
    "\n",
    "def defualt_value_0():\n",
    "    return 0\n",
    "\n",
    "\n",
    "#testing functions\n",
    "\n",
    "def make_test_list(languages):\n",
    "    test_dict=dict()\n",
    "    for lan in languages:\n",
    "        for item in lan.test:\n",
    "            test_dict.update({item: lan.name})\n",
    "    return test_dict\n",
    "\n",
    "def precesion(tp,fp):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def test_checking(inputs):\n",
    "    outputs=[]\n",
    "    tp=0\n",
    "    fp=0\n",
    "    for item in inputs:\n",
    "        p_test=test_case(item,languages,pc.copy())\n",
    "        index=p_test.index(max(p_test))\n",
    "        res=languages[index].name\n",
    "        outputs.append(res)\n",
    "        if res==inputs[item]:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    return outputs,tp,fp\n",
    "\n",
    "\n",
    "def test_validation(inputs):\n",
    "    for item in inputs:\n",
    "        p_test=test_case(item[0],languages,pc.copy())\n",
    "        index=p_test.index(max(p_test))\n",
    "        res=languages[index].name\n",
    "        item.append(languages[index].name)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1.loading dataset and languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### set stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stp_arabic=['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n",
    "stp_english=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "stp_german=['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n",
    "stp_french=['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', 'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'même', 'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', 'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', 'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', 'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', 'à', 'm', 'n', 's', 't', 'y', 'été', 'étée', 'étées', 'étés', 'étant', 'étante', 'étants', 'étantes', 'suis', 'es', 'est', 'sommes', 'êtes', 'sont', 'serai', 'seras', 'sera', 'serons', 'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', 'seraient', 'étais', 'était', 'étions', 'étiez', 'étaient', 'fus', 'fut', 'fûmes', 'fûtes', 'furent', 'sois', 'soit', 'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'fût', 'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', 'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', 'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', 'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', 'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', 'eut', 'eûmes', 'eûtes', 'eurent', 'aie', 'aies', 'ait', 'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'eût', 'eussions', 'eussiez', 'eussent']\n",
    "stp_persian=['و','در','به','از','که','این','را','با','است','برای','آن','یک','خود','تا','کرد','بر','هم','نیز','گفت','وی','شد','دارد','ما','اما','یا','شده','باید','هر','آنها','بود','او','یگر','دو','مورد','شود','کند','وجود','بین','پیش','شده_است','پس','نظر','اگر','همه','یکی','حال','هستند','من','کنند','نیست','باشد','چه','بی','می','بخش','همین','افزود','هایی','دارند','راه','همچنین','روی','داد','بیشتر','بسیار','سه','داشت','چند','سوی','تنها','هیچ','میان','اینکه','شدن','بعد','جدید','ولی','حتی','کردن','برخی','کردند','اول','نه','کرده_است','نسبت','بیش','شما','چنین','طور','افراد','تمام','درباره','بار','بسیاری','کرده','چون','ندارد','دوم','بزرگ','طی','حدود','همان','بدون','البته','آنان','دیگری','خواهد_شد','کنیم','قابل','یعنی','رشد','وارد','کل','ویژه','قبل','براساس','نیاز','گذاری','هنوز','لازم','سازی','بوده_است','چرا','وقتی','گرفت','کم','جای','حالی','تغییر','پیدا','اکنون','تحت','باعث','مدت','فقط','زیادی','تعداد','آیا','بیان','رو','شدند','عدم','کرده_اند','بودن','نوع','بلکه','جاری','دهد','برابر','مهم','بوده','اخیر','مربوط','امر','زیر','گیری','شاید','خصوص','آقای','اثر','کننده','بودند','فکر','کنار','اولین','سوم','سایر','کنید','ضمن','مانند','باز','ممکن','حل','دارای','پی','مثل','اجرا','دور','منظور','کسی','موجب','طول','امکان','آنچه','تعیین','گفته','شوند','جمع','خیلی','علاوه','گونه','تاکنون','رسید','ساله','گرفته','شده_اند','علت','چهار','داشته_باشد','خواهد_بود','طرف','تهیه','تبدیل','مناسب','زیرا','مشخص','نزدیک','جریان','روند','بنابراین','یافت','نخستین','بالا','پنج','ریزی','عالی','چیزی','نخست','بیشتری','ترتیب','شده_بود','خاص','خوبی','خوب','شروع','فرد','کامل','غیر','دهند','آخرین','دادن','جدی','بهترین','شامل','گیرد','بخشی','باشند','تمامی','بهتر','داده_است','حد','نبود','کسانی','داریم','علیه','دانست','ناشی','داشتند','دهه','ایشان','آنجا','گرفته_است','دچار','لحاظ','آنکه','داده','بعضی','هستیم','اند','برداری','نباید','نشست','سهم','همیشه','آمد','اش','وگو','حداقل','طبق','جا','خواهد_کرد','نوعی','چگونه','رفت','هنگام','فوق','روش','ندارند','سعی','بندی','شمار','کلی','کافی','مواجه','همچنان','زیاد','سمت','کوچک','داشته_است','چیز','پشت','آورد','حالا','روبه','دادند','عهده','نیمه','جایی','دیگران','سی','بروز','یکدیگر','آمده_است','جز','کنم','سپس','کنندگان','خودش','همواره','یافته','شان','صرف','رسیدن','چهارم','یابد','متر','ساز','داشته','کرده_بود','باره','نحوه','کردم','تو','شخصی','داشته_باشند','محسوب','پخش','کمی','متفاوت','سراسر','کاملا','داشتن','نظیر','آمده','گروهی','فردی','ع','همچون','خطر','خویش','کدام','دسته','سبب','عین','آوری','متاسفانه','بیرون','دار','ابتدا','شش','افرادی','سالهای','درون','نیستند','یافته_است','پر','خاطرنشان','گاه','جمعی','اغلب','دوباره','لذا','زاده','گردد','اینجا']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "english=Language(\"English\")\n",
    "german=Language(\"German\")\n",
    "french=Language(\"French\")\n",
    "persian=Language(\"Persian\")\n",
    "arabic=Language(\"Arabic\")\n",
    "\n",
    "english.load_dataset(\"/Users/ngn/Desktop/english.txt\")\n",
    "german.load_dataset(\"/Users/ngn/Desktop/german.txt\")\n",
    "french.load_dataset(\"/Users/ngn/Desktop/france.txt\")\n",
    "persian.load_dataset(\"/Users/ngn/Desktop/farsi.txt\")\n",
    "arabic.load_dataset(\"/Users/ngn/Desktop/arabic.txt\")\n",
    "\n",
    "english.set_stop_word(stp_english)\n",
    "french.set_stop_word(stp_french)\n",
    "german.set_stop_word(stp_german)\n",
    "persian.set_stop_word(stp_persian)\n",
    "arabic.set_stop_word(stp_arabic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2.initialize dataset and languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=[english,german,french,persian,arabic]\n",
    "train_lens=[]\n",
    "features=set()\n",
    "for l in languages:\n",
    "    l.seprate_train_test_set()\n",
    "    l.preproccessing()\n",
    "    l.create_features_dict(defualt_value_0)\n",
    "    train_lens.append(int(len(l.train)))\n",
    "    features=features.union(set(l.features_dict))\n",
    "    \n",
    "pc=probability_class(train_lens)# probability of each class in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a list of tess cases\n",
    "\n",
    "test_list=make_test_list(languages)\n",
    "outputs,tp,fp=test_checking(test_list)\n",
    "\n",
    "#precesion(tp,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for 1 test case\n",
    "\n",
    "test_string=\"hello language detector!\"\n",
    "pt=test_case(test_string,languages,pc.copy())# probability of test case in a list\n",
    "\n",
    "index=pt.index(max(pt))\n",
    "detected_language=languages[index].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "document=pd.read_csv('/Users/ngn/Desktop/test.csv', header=0)\n",
    "validation=document.values.tolist()\n",
    "\n",
    "resault_list=test_validation(validation)\n",
    "\n",
    "columns = [\"Id\",\"Category\"]\n",
    "data = [i[0:2] for i in resault_list] \n",
    "pd2 = pd.DataFrame(data=data, columns = columns)\n",
    "pd2.to_csv(\"sample1 - task1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
